{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/juwetta/DLI_Group-B/blob/main/TP074297work/TP074297_FigureRecreate.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Mount to Google Drive**"
      ],
      "metadata": {
        "id": "pu0QxG7635da"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "L0TmHpBRdt1V"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive/', force_remount=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Data Loading & Preprocessing**\n",
        "1. The dataset is cleaned beforehand.\n",
        "2. The dataset is imbalance but it will be split into 80:20 for training and testing instances. This means the imbalance present in the full dataset would also be reflected in the training subset. Moreover, the author used fusion features and ensemble learning to counteract class imbalance, making the model more robust as the dataset is more \"natural\" and closer to real-cases."
      ],
      "metadata": {
        "id": "S9zAlqY_4B1q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import cudf\n",
        "\n",
        "print(\"Pandas version: \", pd.__version__)\n",
        "print(\"CUDF version: \", cudf.__version__)"
      ],
      "metadata": {
        "id": "Usm17FAX5KTa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e06ea20-16f6-4c38-d651-9ffb7a72b6ac"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pandas version:  2.2.2\n",
            "CUDF version:  25.06.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "DATA_URL = \"https://raw.githubusercontent.com/juwetta/DLI_Group-B/main/URL_dataset_clean_balanced.csv\"\n",
        "!wget -O URL_dataset_clean_balanced.csv \"$DATA_URL\"\n",
        "\n",
        "df = pd.read_csv(\"URL_dataset_clean_balanced.csv\")\n",
        "df['type'] = df['type'].replace({'legitimate': 0, 'phishing': 1})\n",
        "ptoc = cudf.DataFrame.from_pandas(df)\n",
        "print(ptoc.head(2))"
      ],
      "metadata": {
        "id": "VvV2Rag_6Epm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "86db0f92-e8b9-44f7-f329-7fcc3a1cc7ab"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-08-23 13:41:54--  https://raw.githubusercontent.com/juwetta/DLI_Group-B/main/URL_dataset_clean_balanced.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 15312637 (15M) [text/plain]\n",
            "Saving to: ‘URL_dataset_clean_balanced.csv’\n",
            "\n",
            "URL_dataset_clean_b 100%[===================>]  14.60M  --.-KB/s    in 0.06s   \n",
            "\n",
            "2025-08-23 13:41:55 (250 MB/s) - ‘URL_dataset_clean_balanced.csv’ saved [15312637/15312637]\n",
            "\n",
            "                                                 url  type\n",
            "0                               http://kitegacc.net/     1\n",
            "1  https://www.electronichouse.com/article/ps3_ad...     0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3187997816.py:5: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
            "  df['type'] = df['type'].replace({'legitimate': 0, 'phishing': 1})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "all_Y = ptoc['type'] #cuDF series\n",
        "all_X = ptoc.drop(columns=['type'])\n",
        "all_X[\"url\"] = all_X[\"url\"].astype(\"category\")\n",
        "\n",
        "print(\"all_X:\")\n",
        "print(all_X.head(5))\n",
        "print(\"\\nall_Y:\")\n",
        "print(all_Y.head(5))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "II-axbXUs5k-",
        "outputId": "655d187a-fb0c-4f17-fc4f-40b5ec3a2aae"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "all_X:\n",
            "                                                 url\n",
            "0                               http://kitegacc.net/\n",
            "1  https://www.electronichouse.com/article/ps3_ad...\n",
            "2      https://www.linkedin.com/in/larrymartinkimpel\n",
            "3  https://www.kansascity.com/2011/03/05/2700249/...\n",
            "4        https://www.en.wikipedia.org/wiki/Dem_Bones\n",
            "\n",
            "all_Y:\n",
            "0    1\n",
            "1    0\n",
            "2    0\n",
            "3    0\n",
            "4    0\n",
            "Name: type, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Splitting Data**"
      ],
      "metadata": {
        "id": "B2Sc3U_x5NXi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from cuml.model_selection import train_test_split\n",
        "SEED=42\n",
        "np.random.seed(SEED)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(all_X, all_Y, test_size=0.2, random_state=SEED, stratify=all_Y)\n",
        "\n",
        "#print(type(X_train))\n"
      ],
      "metadata": {
        "id": "iUsAne766_6i"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Base model Definition**"
      ],
      "metadata": {
        "id": "H02DASBL5ZUX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from xgboost import XGBClassifier\n",
        "from cuml.ensemble import RandomForestClassifier\n",
        "from cuml.linear_model import LogisticRegression\n",
        "from cuml.svm import SVC\n",
        "\n",
        "from cuml.neighbors import KNeighborsClassifier\n",
        "from cupy import asnumpy\n",
        "trees=100\n",
        "# get models\n",
        "def get_models():\n",
        "  models = list()\n",
        "  models.append(XGBClassifier(device=\"cuda\",n_estimators=trees,learning_rate=0.7, enable_categorical=True, tree_method = \"gpu_hist\"))\n",
        "  models.append(SVC(probability=True))\n",
        "  models.append(KNeighborsClassifier())\n",
        "  models.append(LogisticRegression())\n",
        "  models.append(RandomForestClassifier(n_estimators=trees))\n",
        "\n",
        "  return models\n",
        "\n",
        "models = get_models()"
      ],
      "metadata": {
        "id": "oGy10c9y7htu"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def metrics_cal(conf_mat):\n",
        "  print(conf_mat)\n",
        "  TP = conf_mat[0][0]\n",
        "  FP = conf_mat[0][1]\n",
        "  FN = conf_mat[1][0]\n",
        "  TN = conf_mat[1][1]\n",
        "\n",
        "  total = TP+FP+TN+FN\n",
        "  TPR = TP/float(TP+FN)\n",
        "  TNR = TN/float(TN+FP)\n",
        "  Precision = TP/float(TP+FP)\n",
        "  f_score = (2*TPR*Precision)/(TPR+Precision)\n",
        "  MCC = ((TP * TN) - (FP * FN)) / math.sqrt((TP + FP) * (TP + FN) * (TN + FP) * (TN + FN))\n",
        "  ACC = (TP + TN) / (total)\n",
        "  print('TPR :=', TPR, 'TNR:=', TNR, 'Precision := ', Precision, 'F_score:=', f_score, 'MCC := ', MCC, 'ACC := ', ACC)"
      ],
      "metadata": {
        "id": "V13KvLiz7oYY"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def to_numpy_safe(x):\n",
        "    \"\"\"\n",
        "    Convert cudf.Series, cuml.CumlArray, or cupy arrays to numpy.\n",
        "    \"\"\"\n",
        "    import cupy as cp\n",
        "    import numpy as np\n",
        "    if hasattr(x, \"to_pandas\"):\n",
        "        x = x.to_pandas()\n",
        "    if hasattr(x, \"values\") and hasattr(x.values, \"get\"):  # cuDF -> cuPy\n",
        "        return x.values.get()\n",
        "    if \"cupy\" in type(x).__module__:\n",
        "        return cp.asnumpy(x)\n",
        "    if hasattr(x, \"to_numpy\"):\n",
        "        return x.to_numpy()\n",
        "    return np.asarray(x)"
      ],
      "metadata": {
        "id": "u5n9htBXe7NQ"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from numpy import hstack\n",
        "from numpy import vstack\n",
        "from numpy import asarray\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "\n",
        "# make predictions with stacked model\n",
        "def super_learner_predictions(X, models, meta_model):\n",
        "    meta_X = []\n",
        "\n",
        "    for model in models:\n",
        "        yhat = model.predict_proba(X)\n",
        "        yhat = to_numpy_safe(yhat)\n",
        "        meta_X.append(yhat)\n",
        "\n",
        "    # stack predictions from all base models horizontally\n",
        "    meta_X = np.hstack(meta_X)\n",
        "\n",
        "    # predict with meta model\n",
        "    return meta_model.predict(meta_X)\n",
        "\n",
        "\n",
        "# evaluate a list of models on a dataset\n",
        "def evaluate_models(X, y, models):\n",
        "    for model in models:\n",
        "        # Need to handle sparse vs dense input for different models if necessary\n",
        "        yhat = model.predict(X)\n",
        "        yhat_np = yhat.to_numpy() if hasattr(yhat, 'to_numpy') else yhat\n",
        "        y_np = y # y is now a numpy array, no need for to_numpy()\n",
        "        acc = accuracy_score(y_np, yhat_np)\n",
        "        print(metrics_cal(confusion_matrix(y_np, yhat_np)))\n",
        "        print('%s: %.3f' % (model.__class__.__name__, acc * 100))\n",
        "\n",
        "# fit a meta model\n",
        "def fit_meta_model(X, y):\n",
        "    model = LogisticRegression()\n",
        "    model.fit(X, y)\n",
        "    return model\n",
        "\n",
        "# fit all base models on the training dataset\n",
        "def fit_base_models(X, y, models):\n",
        "    for model in models:\n",
        "        model.fit(X, y)\n",
        "\n",
        "import numpy as np\n",
        "import cudf\n",
        "from sklearn.model_selection import KFold\n",
        "import xgboost as xgb\n",
        "\n",
        "def preprocess_dataframe(df):\n",
        "    \"\"\"\n",
        "    Ensure DataFrame has only valid dtypes for XGBoost.\n",
        "    - Convert object/string columns to categorical codes (integers).\n",
        "    \"\"\"\n",
        "    for col in df.columns:\n",
        "        if df[col].dtype == 'object':\n",
        "            df[col] = df[col].astype('category')\n",
        "        if str(df[col].dtype) == 'category':\n",
        "            df[col] = df[col].cat.codes.astype('int32')\n",
        "    return df\n",
        "\n",
        "# def get_out_of_fold_predictions(X_train, Y_train, models, n_splits=10):\n",
        "#     meta_X, meta_y = [], []\n",
        "#     kfold = KFold(n_splits=n_splits, shuffle=True, random_state=SEED)\n",
        "\n",
        "#     # preprocess once at start\n",
        "#     X_train = preprocess_dataframe(X_train)\n",
        "\n",
        "#     for train_ix, test_ix in kfold.split(X_train):\n",
        "#         fold_yhats = []\n",
        "\n",
        "#         # get train/test split\n",
        "#         train_X, test_X = X_train.iloc[train_ix], X_train.iloc[test_ix]\n",
        "#         train_y, test_y = Y_train[train_ix], Y_train[test_ix]\n",
        "\n",
        "#         # ensure numpy labels\n",
        "#         train_y = to_numpy_safe(train_y)\n",
        "#         test_y = to_numpy_safe(test_y)\n",
        "\n",
        "#         meta_y.extend(test_y.tolist())\n",
        "\n",
        "#         # fit and predict with each model\n",
        "#         for model in models:\n",
        "#             model.fit(train_X, train_y)\n",
        "#             yhat = model.predict_proba(test_X)\n",
        "\n",
        "#             # convert predictions to numpy\n",
        "#             if hasattr(yhat, \"to_numpy\"):\n",
        "#                 yhat = yhat.to_numpy()\n",
        "#             elif hasattr(yhat, \"to_output\"):\n",
        "#                 yhat = yhat.to_output(\"numpy\")\n",
        "#             elif hasattr(yhat, \"get\"):\n",
        "#                 yhat = yhat.get()\n",
        "#             else:\n",
        "#                 yhat = np.asarray(yhat)\n",
        "\n",
        "#             fold_yhats.append(yhat)\n",
        "\n",
        "#         # stack predictions horizontally for this fold\n",
        "#         fold_stacked = np.hstack(fold_yhats)\n",
        "#         meta_X.append(fold_stacked)\n",
        "\n",
        "#     # stack all folds vertically\n",
        "#     stacked_meta_X = np.vstack(meta_X)\n",
        "#     return stacked_meta_X, np.asarray(meta_y)\n",
        "\n",
        "\n",
        "# # Example usage\n",
        "# print('Train', X_train.shape, y_train.shape, 'Test', X_test.shape, y_test.shape)\n",
        "\n",
        "# meta_X, meta_y = get_out_of_fold_predictions(X_train, y_train, models)\n",
        "# print('Meta ', meta_X.shape, meta_y.shape)\n"
      ],
      "metadata": {
        "id": "AhET3CvsDbKA"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import KFold\n",
        "import numpy as np\n",
        "import cudf\n",
        "\n",
        "def get_out_of_fold_predictions(X, y, models, n_splits=5):\n",
        "    \"\"\"\n",
        "    Perform OOF predictions for stacking using cuDF + cuML.\n",
        "    Always returns NumPy indices from KFold for consistency.\n",
        "    \"\"\"\n",
        "\n",
        "    kfold = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
        "\n",
        "    # Store OOF preds in CPU memory\n",
        "    meta_X = np.zeros((len(y), len(models)))\n",
        "    y = cudf.Series(y)   # ensure cuDF\n",
        "    meta_y = y.to_numpy()   # convert once for sklearn metrics later\n",
        "\n",
        "    for model_idx, model in enumerate(models):\n",
        "        for train_idx, valid_idx in kfold.split(X.to_pandas(), y.to_pandas()):\n",
        "            # Use NumPy indices → slice cuDF\n",
        "            X_train, X_valid = X.iloc[train_idx], X.iloc[valid_idx]\n",
        "            y_train, y_valid = y.iloc[train_idx], y.iloc[valid_idx]\n",
        "\n",
        "            # Fit GPU model\n",
        "            model.fit(X_train, y_train)\n",
        "\n",
        "            # Predict\n",
        "            preds = model.predict(X_valid)\n",
        "\n",
        "            # Force preds → NumPy\n",
        "            if hasattr(preds, \"get\"):          # CumlArray\n",
        "                preds = preds.get()\n",
        "            elif hasattr(preds, \"to_numpy\"):   # cuDF Series\n",
        "                preds = preds.to_numpy()\n",
        "\n",
        "            # Store preds into NumPy array\n",
        "            meta_X[valid_idx, model_idx] = preds\n",
        "\n",
        "    return meta_X, meta_y\n",
        "\n",
        "\n",
        "    # Example usage\n",
        "print('Train', X_train.shape, y_train.shape, 'Test', X_test.shape, y_test.shape)\n",
        "\n",
        "meta_X, meta_y = get_out_of_fold_predictions(X_train, y_train, models)\n",
        "print('Meta ', meta_X.shape, meta_y.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 388
        },
        "id": "gS-UyOhJj7iP",
        "outputId": "7fad9e51-bc91-447a-9917-939add4eb9c1"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train (167101, 1) (167101,) Test (41775, 1) (41775,)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "DataFrame.dtypes for data must be int, float, bool or category. When categorical type is supplied, the experimental DMatrix parameter`enable_categorical` must be set to `True`.  Invalid columns:url: object",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-775327918.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Train'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Test'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m \u001b[0mmeta_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeta_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_out_of_fold_predictions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Meta '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeta_X\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeta_y\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-775327918.py\u001b[0m in \u001b[0;36mget_out_of_fold_predictions\u001b[0;34m(X, y, models, n_splits)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0;31m# Fit GPU model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;31m# Predict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/xgboost/core.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    727\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    728\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 729\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    730\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    731\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/xgboost/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, base_margin, eval_set, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights)\u001b[0m\n\u001b[1;32m   1662\u001b[0m                 \u001b[0mxgb_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1663\u001b[0m             )\n\u001b[0;32m-> 1664\u001b[0;31m             train_dmatrix, evals = _wrap_evaluation_matrices(\n\u001b[0m\u001b[1;32m   1665\u001b[0m                 \u001b[0mmissing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1666\u001b[0m                 \u001b[0mX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/xgboost/sklearn.py\u001b[0m in \u001b[0;36m_wrap_evaluation_matrices\u001b[0;34m(missing, X, y, group, qid, sample_weight, base_margin, feature_weights, eval_set, sample_weight_eval_set, base_margin_eval_set, eval_group, eval_qid, create_dmatrix, enable_categorical, feature_types)\u001b[0m\n\u001b[1;32m    626\u001b[0m     \"\"\"Convert array_like evaluation matrices into DMatrix.  Perform validation on the\n\u001b[1;32m    627\u001b[0m     way.\"\"\"\n\u001b[0;32m--> 628\u001b[0;31m     train_dmatrix = create_dmatrix(\n\u001b[0m\u001b[1;32m    629\u001b[0m         \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    630\u001b[0m         \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/xgboost/sklearn.py\u001b[0m in \u001b[0;36m_create_dmatrix\u001b[0;34m(self, ref, **kwargs)\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_can_use_qdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree_method\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbooster\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"gblinear\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1136\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1137\u001b[0;31m                 return QuantileDMatrix(\n\u001b[0m\u001b[1;32m   1138\u001b[0m                     \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mref\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mref\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnthread\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_bin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_bin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1139\u001b[0m                 )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/xgboost/core.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    727\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    728\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 729\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    730\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    731\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/xgboost/core.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, label, weight, base_margin, missing, silent, feature_names, feature_types, nthread, max_bin, ref, group, qid, label_lower_bound, label_upper_bound, feature_weights, enable_categorical, max_quantile_batches, data_split_mode)\u001b[0m\n\u001b[1;32m   1612\u001b[0m                 )\n\u001b[1;32m   1613\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1614\u001b[0;31m         self._init(\n\u001b[0m\u001b[1;32m   1615\u001b[0m             \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1616\u001b[0m             \u001b[0mref\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mref\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/xgboost/core.py\u001b[0m in \u001b[0;36m_init\u001b[0;34m(self, data, ref, enable_categorical, max_quantile_blocks, **meta)\u001b[0m\n\u001b[1;32m   1676\u001b[0m             \u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbyref\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1677\u001b[0m         )\n\u001b[0;32m-> 1678\u001b[0;31m         \u001b[0mit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1679\u001b[0m         \u001b[0;31m# delay check_call to throw intermediate exception first\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1680\u001b[0m         \u001b[0m_check_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/xgboost/core.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    570\u001b[0m             \u001b[0mexc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    571\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 572\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mexc\u001b[0m  \u001b[0;31m# pylint: disable=raising-bad-type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__del__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/xgboost/core.py\u001b[0m in \u001b[0;36m_handle_exception\u001b[0;34m(self, fn, dft_ret)\u001b[0m\n\u001b[1;32m    551\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 553\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    554\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    555\u001b[0m             \u001b[0;31m# Defer the exception in order to return 0 and stop the iteration.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/xgboost/core.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    638\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_temporary_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    639\u001b[0m         \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 640\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    641\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    642\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mabstractmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/xgboost/data.py\u001b[0m in \u001b[0;36mnext\u001b[0;34m(self, input_data)\u001b[0m\n\u001b[1;32m   1652\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1653\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mit\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1654\u001b[0;31m         \u001b[0minput_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1655\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1656\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/xgboost/core.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    727\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    728\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 729\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    730\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    731\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/xgboost/core.py\u001b[0m in \u001b[0;36minput_data\u001b[0;34m(data, feature_names, feature_types, **kwargs)\u001b[0m\n\u001b[1;32m    618\u001b[0m                 \u001b[0mnew\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcat_codes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_types\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_temporary_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m                 new, cat_codes, feature_names, feature_types = _proxy_transform(\n\u001b[0m\u001b[1;32m    621\u001b[0m                     \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m                     \u001b[0mfeature_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/xgboost/data.py\u001b[0m in \u001b[0;36m_proxy_transform\u001b[0;34m(data, feature_names, feature_types, enable_categorical)\u001b[0m\n\u001b[1;32m   1668\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fsproxy_fast\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1669\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_cudf_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_is_cudf_ser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1670\u001b[0;31m         return _transform_cudf_df(\n\u001b[0m\u001b[1;32m   1671\u001b[0m             \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_types\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menable_categorical\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1672\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/xgboost/data.py\u001b[0m in \u001b[0;36m_transform_cudf_df\u001b[0;34m(data, feature_names, feature_types, enable_categorical)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1101\u001b[0m     ):\n\u001b[0;32m-> 1102\u001b[0;31m         \u001b[0m_invalid_dataframe_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m     \u001b[0;31m# handle feature names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/xgboost/data.py\u001b[0m in \u001b[0;36m_invalid_dataframe_dtype\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m    370\u001b[0m     \u001b[0mtype_err\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"DataFrame.dtypes for data must be int, float, bool or category.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    371\u001b[0m     \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"\"\"{type_err} {_ENABLE_CAT_ERR} {err}\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 372\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    373\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    374\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: DataFrame.dtypes for data must be int, float, bool or category. When categorical type is supplied, the experimental DMatrix parameter`enable_categorical` must be set to `True`.  Invalid columns:url: object"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Fit Base Models***"
      ],
      "metadata": {
        "id": "HFlr9IZLi4sb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fit_base_models(X_train, y_train, models)"
      ],
      "metadata": {
        "id": "79lK7bh6Dmvz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Super Learner shape handling:"
      ],
      "metadata": {
        "id": "kYnCmuznfxbD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def stack_meta_features(models, X):\n",
        "    feats = []\n",
        "    for m in models:\n",
        "        if hasattr(m, \"predict_proba\"):\n",
        "            proba = m.predict_proba(X)\n",
        "            proba = to_numpy_safe(proba)   # ensure NumPy\n",
        "            # for binary classification, take positive class column\n",
        "            if proba.ndim == 2 and proba.shape[1] > 1:\n",
        "                p1 = proba[:, 1]\n",
        "            else:\n",
        "                p1 = proba.ravel()\n",
        "            feats.append(p1.reshape(-1, 1))\n",
        "        else:\n",
        "            preds = m.predict(X)\n",
        "            preds = to_numpy_safe(preds)   # ensure NumPy\n",
        "            feats.append(preds.reshape(-1, 1))\n",
        "    return np.hstack(feats)\n"
      ],
      "metadata": {
        "id": "JmeM1LHAft3Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "meta_X = stack_meta_features(models, X_train)\n",
        "meta_model = fit_meta_model(meta_X, meta_y)"
      ],
      "metadata": {
        "id": "Fuhfky6bDtXw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Evaluate model**"
      ],
      "metadata": {
        "id": "vMVkfh1g5jKx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "#evaluate_models(X_test, y_test, models)"
      ],
      "metadata": {
        "id": "ka0A4toKDxCR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def super_learner_predictions(X, models, meta_model):\n",
        "    meta_X = []\n",
        "\n",
        "    for model in models:\n",
        "        yhat = model.predict_proba(X)\n",
        "        yhat = to_numpy_safe(yhat)\n",
        "        meta_X.append(yhat)\n",
        "\n",
        "    # stack predictions from all base models horizontally\n",
        "    meta_X = np.hstack(meta_X)\n",
        "\n",
        "    # predict with meta model\n",
        "    return meta_model.predict(meta_X)\n"
      ],
      "metadata": {
        "id": "mw5Yg0-3ED8c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluate meta model\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "print(X_test.dtypes)\n",
        "yhat = super_learner_predictions(X_test, models, meta_model)\n",
        "\n",
        "# handle predict_proba case\n",
        "if yhat.ndim == 2:\n",
        "    yhat = np.argmax(yhat, axis=1)\n",
        "\n",
        "yhat_np = to_numpy_safe(yhat)\n",
        "y_np = to_numpy_safe(y_test)\n",
        "\n",
        "superlearner_acc = accuracy_score(y_np, yhat_np) * 100\n",
        "print(metrics_cal(confusion_matrix(y_np, yhat_np)))\n",
        "print('Super Learner: %.3f' % superlearner_acc)\n",
        "\n"
      ],
      "metadata": {
        "id": "pR06I1tIEJ80"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}