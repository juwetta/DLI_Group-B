{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/juwetta/DLI_Group-B/blob/main/TP074297work/TP074297_FigureRecreate.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Mount to Google Drive**"
      ],
      "metadata": {
        "id": "pu0QxG7635da"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L0TmHpBRdt1V",
        "outputId": "e5d66890-fe3e-47a6-dda1-adc899347d7b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/', force_remount=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Download Dataset**\n",
        "1. The dataset is cleaned beforehand.\n",
        "2. The dataset is imbalance but it will be split into 80:20 for training and testing instances. This means the imbalance present in the full dataset would also be reflected in the training subset. Moreover, the author used fusion features and ensemble learning to counteract class imbalance, making the model more robust as the dataset is more \"natural\" and closer to real-cases."
      ],
      "metadata": {
        "id": "S9zAlqY_4B1q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import cudf\n",
        "\n",
        "print(\"Pandas version: \", pd.__version__)\n",
        "print(\"CUDF version: \", cudf.__version__)"
      ],
      "metadata": {
        "id": "Usm17FAX5KTa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b3bfc344-911c-4cec-b160-295f9cec8faa"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pandas version:  2.2.2\n",
            "CUDF version:  25.06.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "DATA_URL = \"https://raw.githubusercontent.com/juwetta/DLI_Group-B/main/URL_dataset_clean_balanced.csv\"\n",
        "!wget -O URL_dataset_clean_balanced.csv \"$DATA_URL\"\n",
        "\n",
        "df = pd.read_csv(\"URL_dataset_clean_balanced.csv\")\n",
        "ptoc = cudf.DataFrame.from_pandas(df)\n",
        "print(ptoc.head(2))"
      ],
      "metadata": {
        "id": "VvV2Rag_6Epm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b03ba1d0-3e2f-4c12-8749-03db6eda9062"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-08-21 14:47:14--  https://raw.githubusercontent.com/juwetta/DLI_Group-B/main/URL_dataset_clean_balanced.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 15312637 (15M) [text/plain]\n",
            "Saving to: ‘URL_dataset_clean_balanced.csv’\n",
            "\n",
            "\r          URL_datas   0%[                    ]       0  --.-KB/s               \rURL_dataset_clean_b 100%[===================>]  14.60M  --.-KB/s    in 0.04s   \n",
            "\n",
            "2025-08-21 14:47:14 (406 MB/s) - ‘URL_dataset_clean_balanced.csv’ saved [15312637/15312637]\n",
            "\n",
            "                                                 url        type\n",
            "0                               http://kitegacc.net/    phishing\n",
            "1  https://www.electronichouse.com/article/ps3_ad...  legitimate\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "all_X = ptoc.iloc[:, :-1]\n",
        "all_Y = ptoc.iloc[:, 1]"
      ],
      "metadata": {
        "id": "sRpvjFQ46phJ"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Set hyper-parameters**"
      ],
      "metadata": {
        "id": "B2Sc3U_x5NXi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from cuml.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "np.random.seed(42)\n",
        "SEED=88\n",
        "train_X, test_X, train_Y, test_Y = train_test_split(all_X, all_Y, train_size=0.8, random_state=SEED)\n",
        "\n"
      ],
      "metadata": {
        "id": "iUsAne766_6i"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Train Model**"
      ],
      "metadata": {
        "id": "H02DASBL5ZUX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_models():\n",
        "  models = list()\n",
        "  models.append(XGBClassifier(device=\"cuda\",n_estimators=trees,learning_rate=0.7))\n",
        "  models.append(SVC(probability=True))\n",
        "  models.append(KNeighborsClassifier())\n",
        "  models.append(LogisticRegression())\n",
        "  models.append(RandomForestClassifier(n_estimators=trees))\n",
        "\n",
        "  return models\n",
        "\n",
        "from xgboost import XGBClassifier\n",
        "from cuml.ensemble import RandomForestClassifier\n",
        "from cuml.linear_model import LogisticRegression\n",
        "from cuml.svm import SVC\n",
        "\n",
        "from cuml.neighbors import KNeighborsClassifier\n",
        "from cupy import asnumpy\n",
        "trees=100\n",
        "# get models\n",
        "models = get_models()"
      ],
      "metadata": {
        "id": "oGy10c9y7htu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def metrics_cal(conf_mat):\n",
        "  print(conf_mat)\n",
        "  TP = conf_mat[0][0]\n",
        "  FP = conf_mat[0][1]\n",
        "  FN = conf_mat[1][0]\n",
        "  TN = conf_mat[1][1]\n",
        "\n",
        "  total = TP+FP+TN+FN\n",
        "  TPR = TP/float(TP+FN)\n",
        "  TNR = TN/float(TN+FP)\n",
        "  Precision = TP/float(TP+FP)\n",
        "  f_score = (2*TPR*Precision)/(TPR+Precision)\n",
        "  MCC = ((TP * TN) - (FP * FN)) / math.sqrt((TP + FP) * (TP + FN) * (TN + FP) * (TN + FN))\n",
        "  ACC = (TP + TN) / (total)\n",
        "  print('TPR :=', TPR, 'TNR:=', TNR, 'Precision := ', Precision, 'F_score:=', f_score, 'MCC := ', MCC, 'ACC := ', ACC)"
      ],
      "metadata": {
        "id": "V13KvLiz7oYY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from numpy import hstack\n",
        "from numpy import vstack\n",
        "from numpy import asarray\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import confusion_matrix\n",
        "np.random.seed(88)\n",
        "\n",
        "# make predictions with stacked model\n",
        "def super_learner_predictions(X, models, meta_model):\n",
        "    meta_X = list()\n",
        "    for model in models:\n",
        "        yhat = model.predict_proba(X)\n",
        "        meta_X.append(yhat)\n",
        "    #meta_X = hstack(meta_X)\n",
        "    meta_X_np = [x.to_numpy() if hasattr(x, 'to_numpy') else x for x in meta_X]\n",
        "    meta_X = np.hstack(meta_X_np)\n",
        "    # predict\n",
        "    return meta_model.predict(meta_X)\n",
        "\n",
        "# evaluate a list of models on a dataset\n",
        "def evaluate_models(X, y, models):\n",
        "    for model in models:\n",
        "        yhat = model.predict(X)\n",
        "        yhat_np = yhat.to_numpy() if hasattr(yhat, 'to_numpy') else yhat\n",
        "        y_np = y.to_numpy() if hasattr(y, 'to_numpy') else y\n",
        "        acc = accuracy_score(y_np, yhat_np)\n",
        "        print(metrics_cal(confusion_matrix(y_np, yhat_np)))\n",
        "        print('%s: %.3f' % (model.__class__.__name__, acc * 100))\n",
        "\n",
        "# fit a meta model\n",
        "def fit_meta_model(X, y):\n",
        "    model = LogisticRegression()\n",
        "    model.fit(X, y)\n",
        "    return model\n",
        "\n",
        "# fit all base models on the training dataset\n",
        "def fit_base_models(X, y, models):\n",
        "    for model in models:\n",
        "        model.fit(X, y)\n",
        "\n",
        "# collect out of fold predictions form k-fold cross validation\n",
        "def get_out_of_fold_predictions(X_train, Y_train, models):\n",
        "    import pdb\n",
        "    meta_X, meta_y = list(), list()\n",
        "    # define split of data\n",
        "    kfold = KFold(n_splits=10, shuffle=True,random_state=88)\n",
        "    # enumerate splits\n",
        "    for train_ix, test_ix in kfold.split(X_train):\n",
        "        #pdb.set_trace()\n",
        "        fold_yhats = list()\n",
        "        # get data\n",
        "        train_X, test_X = X_train.iloc[train_ix], X_train.iloc[test_ix]\n",
        "        train_y, test_y = Y_train.iloc[train_ix], Y_train.iloc[test_ix]\n",
        "        #meta_y.extend(test_y)\n",
        "        meta_y.extend(test_y)\n",
        "        # fit and make predictions with each sub-model\n",
        "        for model in models:\n",
        "            model.fit(train_X, train_y)\n",
        "            yhat = model.predict_proba(test_X)\n",
        "            # store columns\n",
        "            fold_yhats.append(yhat)\n",
        "        fold_yhats_np = [x.to_numpy() if hasattr(x, 'to_numpy') else x for x in fold_yhats]\n",
        "        meta_X.append(np.hstack(fold_yhats_np))\n",
        "        # store fold yhats as columns\n",
        "        #meta_X.append(hstack(fold_yhats))\n",
        "        meta_X_np = [x.to_numpy() if hasattr(x, 'to_numpy') else x for x in meta_X]\n",
        "        stacked_meta_X = np.vstack(meta_X_np)\n",
        "    return stacked_meta_X, asarray(meta_y)\n",
        "\n",
        "print('Train', X_train.shape, y_train.shape, 'Test', X_test.shape, y_test.shape)\n",
        "\n",
        "meta_X, meta_y = get_out_of_fold_predictions(X_train, y_train, models)\n",
        "print('Meta ', meta_X.shape, meta_y.shape)"
      ],
      "metadata": {
        "id": "AhET3CvsDbKA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fit_base_models(X_train, y_train, models)"
      ],
      "metadata": {
        "id": "79lK7bh6Dmvz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "meta_model = fit_meta_model(meta_X, meta_y)"
      ],
      "metadata": {
        "id": "Fuhfky6bDtXw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Metrics, Plots and Statistical Tests**"
      ],
      "metadata": {
        "id": "vMVkfh1g5jKx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "evaluate_models(X_test, y_test, models)"
      ],
      "metadata": {
        "id": "ka0A4toKDxCR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def super_learner_predictions(X, models, meta_model):\n",
        "    meta_X = list()\n",
        "    for model in models:\n",
        "        yhat = model.predict_proba(X)\n",
        "        meta_X.append(yhat)\n",
        "    #meta_X = hstack(meta_X)\n",
        "    meta_X_np = [x.to_numpy() if hasattr(x, 'to_numpy') else x for x in meta_X]\n",
        "    meta_X = np.hstack(meta_X_np)\n",
        "    # predict\n",
        "    return meta_model.predict(meta_X)"
      ],
      "metadata": {
        "id": "mw5Yg0-3ED8c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluate meta model\n",
        "yhat = super_learner_predictions(X_test, models, meta_model)\n",
        "yhat_np = yhat.to_numpy() if hasattr(yhat, 'to_numpy') else yhat\n",
        "y_np = y_test.to_numpy() if hasattr(y_test, 'to_numpy') else y_test\n",
        "\n",
        "\n",
        "superlearner_acc = accuracy_score(y_np, yhat_np) * 100\n",
        "print(metrics_cal(confusion_matrix(y_np, yhat_np)))\n",
        "print('Super Learner: %.3f' % superlearner_acc)"
      ],
      "metadata": {
        "id": "pR06I1tIEJ80"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}