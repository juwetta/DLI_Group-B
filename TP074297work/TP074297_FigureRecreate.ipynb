{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/juwetta/DLI_Group-B/blob/main/TP074297work/TP074297_FigureRecreate.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Mount to Google Drive**"
      ],
      "metadata": {
        "id": "pu0QxG7635da"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L0TmHpBRdt1V",
        "outputId": "38d2f6ee-c735-4ef7-f6a0-5e016366e84a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/', force_remount=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Download Dataset**\n",
        "1. The dataset is cleaned beforehand.\n",
        "2. The dataset is imbalance but it will be split into 80:20 for training and testing instances. This means the imbalance present in the full dataset would also be reflected in the training subset. Moreover, the author used fusion features and ensemble learning to counteract class imbalance, making the model more robust as the dataset is more \"natural\" and closer to real-cases."
      ],
      "metadata": {
        "id": "S9zAlqY_4B1q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import cudf\n",
        "\n",
        "print(\"Pandas version: \", pd.__version__)\n",
        "print(\"CUDF version: \", cudf.__version__)"
      ],
      "metadata": {
        "id": "Usm17FAX5KTa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "70803568-6a6a-45b3-d6ab-1924ab47df72"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pandas version:  2.2.2\n",
            "CUDF version:  25.06.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "DATA_URL = \"https://raw.githubusercontent.com/juwetta/DLI_Group-B/main/URL_dataset_clean_balanced.csv\"\n",
        "!wget -O URL_dataset_clean_balanced.csv \"$DATA_URL\"\n",
        "\n",
        "df = pd.read_csv(\"URL_dataset_clean_balanced.csv\")\n",
        "df['type'] = df['type'].replace({'legitimate': 0, 'phishing': 1})\n",
        "ptoc = cudf.DataFrame.from_pandas(df)\n",
        "print(ptoc.head(2))"
      ],
      "metadata": {
        "id": "VvV2Rag_6Epm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d455d9c7-75de-47e8-835e-3aa2744ae0d0"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-08-21 15:14:44--  https://raw.githubusercontent.com/juwetta/DLI_Group-B/main/URL_dataset_clean_balanced.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 15312637 (15M) [text/plain]\n",
            "Saving to: ‘URL_dataset_clean_balanced.csv’\n",
            "\n",
            "URL_dataset_clean_b 100%[===================>]  14.60M  --.-KB/s    in 0.04s   \n",
            "\n",
            "2025-08-21 15:14:44 (390 MB/s) - ‘URL_dataset_clean_balanced.csv’ saved [15312637/15312637]\n",
            "\n",
            "0                                      http://kitegacc.net/\n",
            "1         https://www.electronichouse.com/article/ps3_ad...\n",
            "2             https://www.linkedin.com/in/larrymartinkimpel\n",
            "3         https://www.kansascity.com/2011/03/05/2700249/...\n",
            "4               https://www.en.wikipedia.org/wiki/Dem_Bones\n",
            "                                ...                        \n",
            "208871    http://www.apsweb.co.jp/wordpress/ihup/nD/inde...\n",
            "208872                 https://www.theruckus.wordpress.com/\n",
            "208873          http://jambidaily.com/34g3f3g/68k7jh65g.exe\n",
            "208874                 http://ejanla.co/43543r34r/843tf.exe\n",
            "208875                       http://sadovod-gel.ru/43h36n54\n",
            "Name: url, Length: 208876, dtype: object\n",
            "                                                 url  type\n",
            "0                               http://kitegacc.net/     1\n",
            "1  https://www.electronichouse.com/article/ps3_ad...     0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1587427400.py:6: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
            "  df['type'] = df['type'].replace({'legitimate': 0, 'phishing': 1})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "all_X = ptoc.iloc[:, :-1]\n",
        "all_Y = ptoc.iloc[:, 1]"
      ],
      "metadata": {
        "id": "sRpvjFQ46phJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "777ae2d9-62ed-4420-986f-89a5db4a512c"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'cudf.core.dataframe.DataFrame'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Set hyper-parameters**"
      ],
      "metadata": {
        "id": "B2Sc3U_x5NXi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from cuml.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "np.random.seed(42)\n",
        "SEED=88\n",
        "X_train, X_test, y_train, y_test = train_test_split(all_X, all_Y, train_size=0.5, random_state=SEED)\n",
        "\n",
        "print(type(X_train))"
      ],
      "metadata": {
        "id": "iUsAne766_6i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "94060e7c-9942-4c53-c2c1-8bf052115be4"
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'cudf.core.dataframe.DataFrame'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Train Model**"
      ],
      "metadata": {
        "id": "H02DASBL5ZUX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from xgboost import XGBClassifier\n",
        "from cuml.ensemble import RandomForestClassifier\n",
        "from cuml.linear_model import LogisticRegression\n",
        "from cuml.svm import SVC\n",
        "\n",
        "from cuml.neighbors import KNeighborsClassifier\n",
        "from cupy import asnumpy\n",
        "trees=100\n",
        "# get models\n",
        "def get_models():\n",
        "  models = list()\n",
        "  models.append(XGBClassifier(device=\"cuda\",n_estimators=trees,learning_rate=0.7, enable_categorical=True))\n",
        "  models.append(SVC(probability=True))\n",
        "  models.append(KNeighborsClassifier())\n",
        "  models.append(LogisticRegression())\n",
        "  models.append(RandomForestClassifier(n_estimators=trees))\n",
        "\n",
        "  return models\n",
        "\n",
        "models = get_models()"
      ],
      "metadata": {
        "id": "oGy10c9y7htu"
      },
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def metrics_cal(conf_mat):\n",
        "  print(conf_mat)\n",
        "  TP = conf_mat[0][0]\n",
        "  FP = conf_mat[0][1]\n",
        "  FN = conf_mat[1][0]\n",
        "  TN = conf_mat[1][1]\n",
        "\n",
        "  total = TP+FP+TN+FN\n",
        "  TPR = TP/float(TP+FN)\n",
        "  TNR = TN/float(TN+FP)\n",
        "  Precision = TP/float(TP+FP)\n",
        "  f_score = (2*TPR*Precision)/(TPR+Precision)\n",
        "  MCC = ((TP * TN) - (FP * FN)) / math.sqrt((TP + FP) * (TP + FN) * (TN + FP) * (TN + FN))\n",
        "  ACC = (TP + TN) / (total)\n",
        "  print('TPR :=', TPR, 'TNR:=', TNR, 'Precision := ', Precision, 'F_score:=', f_score, 'MCC := ', MCC, 'ACC := ', ACC)"
      ],
      "metadata": {
        "id": "V13KvLiz7oYY"
      },
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from numpy import hstack\n",
        "from numpy import vstack\n",
        "from numpy import asarray\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "import numpy as np\n",
        "np.random.seed(88)\n",
        "\n",
        "# make predictions with stacked model\n",
        "def super_learner_predictions(X, models, meta_model):\n",
        "    meta_X = []\n",
        "\n",
        "    for model in models:\n",
        "        yhat = model.predict_proba(X)\n",
        "        yhat = to_numpy_safe(yhat)\n",
        "        meta_X.append(yhat)\n",
        "\n",
        "    # stack predictions from all base models horizontally\n",
        "    meta_X = np.hstack(meta_X)\n",
        "\n",
        "    # predict with meta model\n",
        "    return meta_model.predict(meta_X)\n",
        "\n",
        "\n",
        "# evaluate a list of models on a dataset\n",
        "def evaluate_models(X, y, models):\n",
        "    for model in models:\n",
        "        # Need to handle sparse vs dense input for different models if necessary\n",
        "        yhat = model.predict(X)\n",
        "        yhat_np = yhat.to_numpy() if hasattr(yhat, 'to_numpy') else yhat\n",
        "        y_np = y # y is now a numpy array, no need for to_numpy()\n",
        "        acc = accuracy_score(y_np, yhat_np)\n",
        "        print(metrics_cal(confusion_matrix(y_np, yhat_np)))\n",
        "        print('%s: %.3f' % (model.__class__.__name__, acc * 100))\n",
        "\n",
        "# fit a meta model\n",
        "def fit_meta_model(X, y):\n",
        "    model = LogisticRegression()\n",
        "    model.fit(X, y)\n",
        "    return model\n",
        "\n",
        "# fit all base models on the training dataset\n",
        "def fit_base_models(X, y, models):\n",
        "    for model in models:\n",
        "        model.fit(X, y)\n",
        "\n",
        "import numpy as np\n",
        "import cudf\n",
        "from sklearn.model_selection import KFold\n",
        "import xgboost as xgb\n",
        "\n",
        "def preprocess_dataframe(df):\n",
        "    \"\"\"\n",
        "    Ensure DataFrame has only valid dtypes for XGBoost.\n",
        "    - Convert object/string columns to categorical codes (integers).\n",
        "    \"\"\"\n",
        "    for col in df.columns:\n",
        "        if df[col].dtype == 'object':\n",
        "            df[col] = df[col].astype('category')\n",
        "        if str(df[col].dtype) == 'category':\n",
        "            df[col] = df[col].cat.codes.astype('int32')\n",
        "    return df\n",
        "\n",
        "def to_numpy_safe(y):\n",
        "    \"\"\"\n",
        "    Convert cudf.Series, cuml.CumlArray, or cupy arrays to numpy.\n",
        "    \"\"\"\n",
        "    if hasattr(y, \"to_numpy\"):            # cudf.Series\n",
        "        return y.to_numpy()\n",
        "    elif hasattr(y, \"to_output\"):         # cuml.CumlArray\n",
        "        return y.to_output(\"numpy\")\n",
        "    elif hasattr(y, \"get\"):               # cupy array\n",
        "        return y.get()\n",
        "    else:                                 # already numpy\n",
        "        return np.asarray(y)\n",
        "\n",
        "def get_out_of_fold_predictions(X_train, Y_train, models, n_splits=10):\n",
        "    meta_X, meta_y = [], []\n",
        "    kfold = KFold(n_splits=n_splits, shuffle=True, random_state=88)\n",
        "\n",
        "    # preprocess once at start\n",
        "    X_train = preprocess_dataframe(X_train)\n",
        "\n",
        "    for train_ix, test_ix in kfold.split(X_train):\n",
        "        fold_yhats = []\n",
        "\n",
        "        # get train/test split\n",
        "        train_X, test_X = X_train.iloc[train_ix], X_train.iloc[test_ix]\n",
        "        train_y, test_y = Y_train[train_ix], Y_train[test_ix]\n",
        "\n",
        "        # ensure numpy labels\n",
        "        train_y = to_numpy_safe(train_y)\n",
        "        test_y = to_numpy_safe(test_y)\n",
        "\n",
        "        meta_y.extend(test_y.tolist())\n",
        "\n",
        "        # fit and predict with each model\n",
        "        for model in models:\n",
        "            model.fit(train_X, train_y)\n",
        "            yhat = model.predict_proba(test_X)\n",
        "\n",
        "            # convert predictions to numpy\n",
        "            if hasattr(yhat, \"to_numpy\"):\n",
        "                yhat = yhat.to_numpy()\n",
        "            elif hasattr(yhat, \"to_output\"):\n",
        "                yhat = yhat.to_output(\"numpy\")\n",
        "            elif hasattr(yhat, \"get\"):\n",
        "                yhat = yhat.get()\n",
        "            else:\n",
        "                yhat = np.asarray(yhat)\n",
        "\n",
        "            fold_yhats.append(yhat)\n",
        "\n",
        "        # stack predictions horizontally for this fold\n",
        "        fold_stacked = np.hstack(fold_yhats)\n",
        "        meta_X.append(fold_stacked)\n",
        "\n",
        "    # stack all folds vertically\n",
        "    stacked_meta_X = np.vstack(meta_X)\n",
        "    return stacked_meta_X, np.asarray(meta_y)\n",
        "\n",
        "\n",
        "# Example usage\n",
        "print('Train', X_train.shape, y_train.shape, 'Test', X_test.shape, y_test.shape)\n",
        "\n",
        "meta_X, meta_y = get_out_of_fold_predictions(X_train, y_train, models)\n",
        "print('Meta ', meta_X.shape, meta_y.shape)\n"
      ],
      "metadata": {
        "id": "AhET3CvsDbKA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "521a0c34-1429-4123-b2d3-f82b5faaace5"
      },
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train (104438, 1) (104438,) Test (104438, 1) (104438,)\n",
            "[2025-08-21 17:07:20.416] [CUML] [warning] L-BFGS line search failed (code 4); stopping at the last valid step\n",
            "[2025-08-21 17:07:21.725] [CUML] [warning] L-BFGS line search failed (code 4); stopping at the last valid step\n",
            "[2025-08-21 17:07:23.072] [CUML] [warning] L-BFGS line search failed (code 4); stopping at the last valid step\n",
            "[2025-08-21 17:07:24.896] [CUML] [warning] L-BFGS line search failed (code 4); stopping at the last valid step\n",
            "[2025-08-21 17:07:26.538] [CUML] [warning] L-BFGS line search failed (code 4); stopping at the last valid step\n",
            "[2025-08-21 17:07:27.853] [CUML] [warning] L-BFGS line search failed (code 4); stopping at the last valid step\n",
            "[2025-08-21 17:07:29.196] [CUML] [warning] L-BFGS line search failed (code 4); stopping at the last valid step\n",
            "[2025-08-21 17:07:30.520] [CUML] [warning] L-BFGS line search failed (code 4); stopping at the last valid step\n",
            "[2025-08-21 17:07:31.814] [CUML] [warning] L-BFGS line search failed (code 4); stopping at the last valid step\n",
            "[2025-08-21 17:07:33.127] [CUML] [warning] L-BFGS line search failed (code 4); stopping at the last valid step\n",
            "Meta  (104438, 10) (104438,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fit_base_models(X_train, y_train, models)"
      ],
      "metadata": {
        "id": "79lK7bh6Dmvz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aef73dcb-c001-47ff-812e-6f7a9079563c"
      },
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-08-21 17:07:35.275] [CUML] [warning] L-BFGS line search failed (code 4); stopping at the last valid step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "meta_model = fit_meta_model(meta_X, meta_y)"
      ],
      "metadata": {
        "id": "Fuhfky6bDtXw"
      },
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Metrics, Plots and Statistical Tests**"
      ],
      "metadata": {
        "id": "vMVkfh1g5jKx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "#evaluate_models(X_test, y_test, models)"
      ],
      "metadata": {
        "id": "ka0A4toKDxCR"
      },
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def super_learner_predictions(X, models, meta_model):\n",
        "    meta_X = []\n",
        "\n",
        "    for model in models:\n",
        "        yhat = model.predict_proba(X)\n",
        "        yhat = to_numpy_safe(yhat)\n",
        "        meta_X.append(yhat)\n",
        "\n",
        "    # stack predictions from all base models horizontally\n",
        "    meta_X = np.hstack(meta_X)\n",
        "\n",
        "    # predict with meta model\n",
        "    return meta_model.predict(meta_X)\n"
      ],
      "metadata": {
        "id": "mw5Yg0-3ED8c"
      },
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluate meta model\n",
        "def preprocess_cudf(df):\n",
        "    \"\"\"\n",
        "    Convert object/string columns in cuDF DataFrame to numeric\n",
        "    (using categorical encoding).\n",
        "    \"\"\"\n",
        "    for col in df.columns:\n",
        "        if df[col].dtype == 'object' or str(df[col].dtype) == 'str':\n",
        "            df[col] = df[col].astype('category').cat.codes\n",
        "    return df\n",
        "\n",
        "# Apply preprocessing to X_train and X_test\n",
        "X_train = preprocess_cudf(X_train)\n",
        "X_test  = preprocess_cudf(X_test)\n",
        "\n",
        "# Now safe to run\n",
        "yhat = super_learner_predictions(X_test, models, meta_model)\n",
        "\n",
        "yhat = super_learner_predictions(X_test, models, meta_model)\n",
        "\n",
        "# handle predict_proba case\n",
        "if yhat.ndim == 2:\n",
        "    yhat = np.argmax(yhat, axis=1)\n",
        "\n",
        "yhat_np = to_numpy_safe(yhat)\n",
        "y_np = to_numpy_safe(y_test)\n",
        "\n",
        "superlearner_acc = accuracy_score(y_np, yhat_np) * 100\n",
        "print(metrics_cal(confusion_matrix(y_np, yhat_np)))\n",
        "print('Super Learner: %.3f' % superlearner_acc)\n"
      ],
      "metadata": {
        "id": "pR06I1tIEJ80",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "29ff6c65-40e3-4613-81bf-4a82a8addf7b"
      },
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[52112    95]\n",
            " [  439 51792]]\n",
            "TPR := 0.9916462103480429 TNR:= 0.998169098232698 Precision :=  0.9981803206466566 F_score:= 0.9949025372763893 MCC :=  0.989795329297855 ACC :=  0.9948869185545491\n",
            "None\n",
            "Super Learner: 99.489\n"
          ]
        }
      ]
    }
  ]
}