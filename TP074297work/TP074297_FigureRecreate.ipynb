{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Mount to Google Drive**"
      ],
      "metadata": {
        "id": "pu0QxG7635da"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L0TmHpBRdt1V",
        "outputId": "b6986513-a51b-415a-eb9e-3ff3cf36d21a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/', force_remount=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Download Dataset**\n",
        "1. The dataset is cleaned beforehand.\n",
        "2. The dataset is imbalance but it will be split into 80:20 for training and testing instances. This means the imbalance present in the full dataset would also be reflected in the training subset. Moreover, the author used fusion features and ensemble learning to counteract class imbalance, making the model more robust as the dataset is more \"natural\" and closer to real-cases."
      ],
      "metadata": {
        "id": "S9zAlqY_4B1q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import math\n",
        "import numpy as np\n",
        "import panda as pd\n",
        "import cudf\n",
        "\n",
        "print(\"Pandas version: \", pd.__version__)\n",
        "print(\"CUDF version: \", cudf.__version__)"
      ],
      "metadata": {
        "id": "Usm17FAX5KTa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"/content/drive/My Drive/DLI Group B/Dataset_Phishdump\")\n",
        "ptoc = cudf.DataFrame.from_pandas(df)\n",
        "print(ptoc.head(2))"
      ],
      "metadata": {
        "id": "VvV2Rag_6Epm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_X = ptoc.iloc[:, :-1]\n",
        "all_Y = ptoc.iloc[:, 921]"
      ],
      "metadata": {
        "id": "sRpvjFQ46phJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Set hyper-parameters**"
      ],
      "metadata": {
        "id": "B2Sc3U_x5NXi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from cuml.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion matrix\n",
        "\n",
        "np.random.seed(42)\n",
        "SEED=88\n",
        "train_X, test_X, train_Y, test_Y = train_test_split(all_X, all_Y, train_size=0.8, random_state=SEED)\n",
        "\n"
      ],
      "metadata": {
        "id": "iUsAne766_6i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Train Model**"
      ],
      "metadata": {
        "id": "H02DASBL5ZUX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_models():\n",
        "  models = list()\n",
        "  models.append(XGBClassifier(device=\"cuda\",n_estimators=trees,learning_rate=0.7))\n",
        "  models.append(SVC(probability=True))\n",
        "  models.append(KNeighborsClassifier())\n",
        "  models.append(LogisticRegression())\n",
        "  models.append(RandomForestClassifier(n_estimators=trees))\n",
        "\n",
        "  return models\n",
        "\n",
        "from xgboost import XGBClassifier\n",
        "from cuml.ensemble import RandomForestClassifier\n",
        "from cuml.linear_model import LogisticRegression\n",
        "from cuml.svm import SVC\n",
        "\n",
        "from cuml.neighbors import KNeighborsClassifier\n",
        "from cupy import asnumpy\n",
        "trees=100\n",
        "# get models\n",
        "models = get_models()"
      ],
      "metadata": {
        "id": "oGy10c9y7htu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def metrics_cal(conf_mat):\n",
        "  print(conf_mat)\n",
        "  TP = conf_mat[0][0]\n",
        "  FP = conf_mat[0][1]\n",
        "  FN = conf_mat[1][0]\n",
        "  TN = conf_mat[1][1]\n",
        "\n",
        "  total = TP+FP+TN+FN\n",
        "  TPR = TP/float(TP+FN)\n",
        "  TNR = TN/float(TN+FP)\n",
        "  Precision = TP/float(TP+FP)\n",
        "  f_score = (2*TPR*Precision)/(TPR+Precision)\n",
        "  MCC = ((TP * TN) - (FP * FN)) / math.sqrt((TP + FP) * (TP + FN) * (TN + FP) * (TN + FN))\n",
        "  ACC = (TP + TN) / (total)\n",
        "  print('TPR :=', TPR, 'TNR:=', TNR, 'Precision := ', Precision, 'F_score:=', f_score, 'MCC := ', MCC, 'ACC := ', ACC)"
      ],
      "metadata": {
        "id": "V13KvLiz7oYY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Metrics, Plots and Statistical Tests**"
      ],
      "metadata": {
        "id": "vMVkfh1g5jKx"
      }
    }
  ]
}