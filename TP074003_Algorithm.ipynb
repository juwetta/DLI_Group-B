{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM2koS/eylMQXlSQnIjHN76",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/juwetta/DLI_Group-B/blob/main/TP074003_Algorithm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mount google drive and import dataset"
      ],
      "metadata": {
        "id": "UPfWlqpFzy1Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loading the Dataset: The dataset is loaded into a pandas DataFrame, and the features (X) and the target variable (y) are extracted. The features appear to include columns 3 to the second-to-last column, and the target variable is the last column."
      ],
      "metadata": {
        "id": "_Z0UPouuW_Lz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing the libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "\n",
        "# Mount your Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Importing the dataset\n",
        "dataset = pd.read_csv('/content/drive/My Drive/Colab Notebooks/Assignment/URL_dataset_clean_balanced.csv')\n",
        "\n",
        "\n",
        "display(dataset)\n"
      ],
      "metadata": {
        "id": "Ynu6faJoW8dK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# V2 autoencoder"
      ],
      "metadata": {
        "id": "dMvHy6TiYC9h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Full leak-free pipeline: URL -> Autoencoder (FE) -> XGBoost (train via xgb.train with early stopping)\n",
        "# Run in Colab (assumes dataset at '/content/drive/My Drive/Colab Notebooks/Assignment/URL_dataset_clean_balanced.csv')\n",
        "\n",
        "# Install if needed (uncomment)\n",
        "# !pip install xgboost tensorflow\n",
        "\n",
        "import os\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from urllib.parse import urlparse\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "import xgboost as xgb\n",
        "\n",
        "# Reproducibility\n",
        "SEED = 42\n",
        "np.random.seed(SEED)\n",
        "tf.random.set_seed(SEED)\n",
        "\n",
        "# ----------- Load dataset -----------\n",
        "DATA_PATH = '/content/drive/My Drive/Colab Notebooks/Assignment/URL_dataset_clean_balanced.csv'\n",
        "df = pd.read_csv(DATA_PATH)\n",
        "\n",
        "# Quick check\n",
        "print(\"Dataset rows:\", len(df))\n",
        "print(\"Columns:\", df.columns.tolist())\n",
        "\n",
        "# ----------- Compact URL feature extraction -----------\n",
        "def extract_url_features(url: str):\n",
        "    # small, robust set of numeric features per URL — cheap & effective\n",
        "    try:\n",
        "        s = str(url)\n",
        "        p = urlparse(s)\n",
        "        host = p.hostname or \"\"\n",
        "        path = p.path or \"\"\n",
        "        query = p.query or \"\"\n",
        "        lower = s.lower()\n",
        "        feats = {\n",
        "            \"url_len\": len(s),\n",
        "            \"host_len\": len(host),\n",
        "            \"path_len\": len(path),\n",
        "            \"query_len\": len(query),\n",
        "            \"n_dots\": lower.count('.'),\n",
        "            \"n_hyphens\": lower.count('-'),\n",
        "            \"n_slash\": lower.count('/'),\n",
        "            \"n_qmark\": lower.count('?'),\n",
        "            \"n_eq\": lower.count('='),\n",
        "            \"n_and\": lower.count('&'),\n",
        "            \"n_at\": lower.count('@'),\n",
        "            \"n_pct\": lower.count('%'),\n",
        "            \"n_digits\": sum(c.isdigit() for c in lower),\n",
        "            \"has_ip\": 1 if re.match(r'^\\d{1,3}(\\.\\d{1,3}){3}$', host) else 0,\n",
        "            \"https\": 1 if lower.startswith('https') else 0,\n",
        "            \"has_www\": 1 if 'www' in host else 0,\n",
        "            \"n_params\": (query.count('&') + (1 if '=' in query else 0)),\n",
        "            \"subdomains\": max(0, len(host.split('.')) - 2) if host else 0,\n",
        "        }\n",
        "        return pd.Series(feats)\n",
        "    except Exception:\n",
        "        # safe fallback of zeros\n",
        "        return pd.Series({\n",
        "            \"url_len\":0,\"host_len\":0,\"path_len\":0,\"query_len\":0,\"n_dots\":0,\"n_hyphens\":0,\n",
        "            \"n_slash\":0,\"n_qmark\":0,\"n_eq\":0,\"n_and\":0,\"n_at\":0,\"n_pct\":0,\"n_digits\":0,\n",
        "            \"has_ip\":0,\"https\":0,\"has_www\":0,\"n_params\":0,\"subdomains\":0\n",
        "        })\n",
        "\n",
        "# Apply to dataset (this may take some time, but it's memory-efficient)\n",
        "print(\"Extracting features from URLs...\")\n",
        "X_feats = df['url'].astype(str).apply(extract_url_features)\n",
        "print(\"Feature extraction done. Shape:\", X_feats.shape)\n",
        "\n",
        "# ----------- Labels -----------\n",
        "label_col = 'type'  # adjust if different\n",
        "le = LabelEncoder()\n",
        "y = le.fit_transform(df[label_col].astype(str))\n",
        "print(\"Classes:\", list(le.classes_))\n",
        "\n",
        "# ----------- Train/Test split (80/20) -----------\n",
        "X_train_raw, X_test_raw, y_train, y_test = train_test_split(\n",
        "    X_feats.values, y, test_size=0.2, random_state=SEED, stratify=y\n",
        ")\n",
        "print(\"Split sizes -- train:\", X_train_raw.shape, \" test:\", X_test_raw.shape)\n",
        "\n",
        "# ----------- Scale (fit on train ONLY) -----------\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train_raw)\n",
        "X_test = scaler.transform(X_test_raw)\n",
        "\n",
        "# ----------- Further split train -> train_sub / val_sub for autoencoder training -----------\n",
        "X_train_sub, X_val_sub = train_test_split(X_train, test_size=0.10, random_state=SEED)  # 10% of train for AE val\n",
        "print(\"AE train/val shapes:\", X_train_sub.shape, X_val_sub.shape)\n",
        "\n",
        "# ----------- Autoencoder (train on X_train_sub, validate on X_val_sub) -----------\n",
        "input_dim = X_train.shape[1]\n",
        "latent_dim = 12   # try 8/12/16; 12 is a good start\n",
        "\n",
        "inputs = tf.keras.Input(shape=(input_dim,))\n",
        "x = layers.Dense(64, activation='relu')(inputs)\n",
        "x = layers.Dense(32, activation='relu')(x)\n",
        "z = layers.Dense(latent_dim, activation='linear', name='latent')(x)\n",
        "x = layers.Dense(32, activation='relu')(z)\n",
        "x = layers.Dense(64, activation='relu')(x)\n",
        "outputs = layers.Dense(input_dim, activation='linear')(x)\n",
        "\n",
        "autoencoder = models.Model(inputs, outputs)\n",
        "autoencoder.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3), loss='mse')\n",
        "\n",
        "early_cb = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=6, restore_best_weights=True, verbose=1)\n",
        "\n",
        "print(\"Training autoencoder...\")\n",
        "autoencoder.fit(\n",
        "    X_train_sub, X_train_sub,\n",
        "    epochs=50,\n",
        "    batch_size=512,\n",
        "    shuffle=True,\n",
        "    validation_data=(X_val_sub, X_val_sub),\n",
        "    callbacks=[early_cb],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# ----------- Extract latent features for train (full) and test -----------\n",
        "encoder = models.Model(inputs, autoencoder.get_layer('latent').output)\n",
        "Z_train = encoder.predict(X_train, batch_size=4096, verbose=0)   # use full train (not just sub)\n",
        "Z_test = encoder.predict(X_test, batch_size=4096, verbose=0)\n",
        "print(\"Latent shapes:\", Z_train.shape, Z_test.shape)\n",
        "\n",
        "# ----------- For XGBoost: create train/validation split on latent space -----------\n",
        "Z_train_sub, Z_val_sub, y_train_sub, y_val_sub = train_test_split(\n",
        "    Z_train, y_train, test_size=0.10, random_state=SEED, stratify=y_train\n",
        ")\n",
        "print(\"XGB train/val shapes:\", Z_train_sub.shape, Z_val_sub.shape)\n",
        "\n",
        "# ----------- Prepare DMatrix and params for xgb.train (works across versions) -----------\n",
        "dtrain = xgb.DMatrix(Z_train_sub, label=y_train_sub)\n",
        "dval = xgb.DMatrix(Z_val_sub, label=y_val_sub)\n",
        "dtest = xgb.DMatrix(Z_test, label=y_test)\n",
        "\n",
        "params = {\n",
        "    \"objective\": \"binary:logistic\" if len(le.classes_)==2 else \"multi:softprob\",\n",
        "    \"eval_metric\": \"logloss\",\n",
        "    \"max_depth\": 6,\n",
        "    \"eta\": 0.05,\n",
        "    \"subsample\": 0.8,\n",
        "    \"colsample_bytree\": 0.8,\n",
        "    \"seed\": SEED,\n",
        "    \"tree_method\": \"hist\"\n",
        "}\n",
        "if len(le.classes_) > 2:\n",
        "    params[\"num_class\"] = len(le.classes_)\n",
        "\n",
        "evals = [(dtrain, \"train\"), (dval, \"val\")]\n",
        "\n",
        "print(\"Training XGBoost with early stopping (xgb.train)...\")\n",
        "xgb_model = xgb.train(\n",
        "    params,\n",
        "    dtrain,\n",
        "    num_boost_round=1000,\n",
        "    evals=evals,\n",
        "    early_stopping_rounds=20,\n",
        "    verbose_eval=50\n",
        ")\n",
        "\n",
        "# ----------- Predict & Evaluate on clean TEST set -----------\n",
        "\n",
        "# Use best_iteration if available, else predict with full model\n",
        "if hasattr(xgb_model, \"best_iteration\") and xgb_model.best_iteration is not None:\n",
        "    y_pred_prob = xgb_model.predict(dtest, iteration_range=(0, xgb_model.best_iteration+1))\n",
        "else:\n",
        "    y_pred_prob = xgb_model.predict(dtest)\n",
        "\n",
        "# For binary vs multiclass:\n",
        "if len(le.classes_) == 2:\n",
        "    y_pred = (y_pred_prob > 0.5).astype(int)\n",
        "else:\n",
        "    y_pred = np.argmax(y_pred_prob, axis=1)\n",
        "\n",
        "print(\"\\n📊 Final Evaluation (Autoencoder + XGB)\")\n",
        "print(\"Accuracy  :\", accuracy_score(y_test, y_pred))\n",
        "print(\"Precision :\", precision_score(y_test, y_pred, average=\"weighted\"))\n",
        "print(\"Recall    :\", recall_score(y_test, y_pred, average=\"weighted\"))\n",
        "print(\"F1-score  :\", f1_score(y_test, y_pred, average=\"weighted\"))\n",
        "\n",
        "# Confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "print(\"Confusion matrix:\\n\", cm)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RASAwTQKYEwv",
        "outputId": "8a68a083-09d4-4a43-b8f7-b0ad80237419"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset rows: 208876\n",
            "Columns: ['url', 'type']\n",
            "Extracting features from URLs...\n",
            "Feature extraction done. Shape: (208876, 18)\n",
            "Classes: ['legitimate', 'phishing']\n",
            "Split sizes -- train: (167100, 18)  test: (41776, 18)\n",
            "AE train/val shapes: (150390, 18) (16710, 18)\n",
            "Training autoencoder...\n",
            "Epoch 1/50\n",
            "\u001b[1m294/294\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 0.4494 - val_loss: 0.0815\n",
            "Epoch 2/50\n",
            "\u001b[1m294/294\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.0733 - val_loss: 0.0455\n",
            "Epoch 3/50\n",
            "\u001b[1m294/294\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - loss: 0.0415 - val_loss: 0.0328\n",
            "Epoch 4/50\n",
            "\u001b[1m294/294\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.0322 - val_loss: 0.0262\n",
            "Epoch 5/50\n",
            "\u001b[1m294/294\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.0284 - val_loss: 0.0227\n",
            "Epoch 6/50\n",
            "\u001b[1m294/294\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - loss: 0.0218 - val_loss: 0.0187\n",
            "Epoch 7/50\n",
            "\u001b[1m294/294\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0184 - val_loss: 0.0175\n",
            "Epoch 8/50\n",
            "\u001b[1m294/294\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0159 - val_loss: 0.0163\n",
            "Epoch 9/50\n",
            "\u001b[1m294/294\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - loss: 0.0142 - val_loss: 0.0152\n",
            "Epoch 10/50\n",
            "\u001b[1m294/294\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - loss: 0.0142 - val_loss: 0.0131\n",
            "Epoch 11/50\n",
            "\u001b[1m294/294\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.0154 - val_loss: 0.0130\n",
            "Epoch 12/50\n",
            "\u001b[1m294/294\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - loss: 0.0166 - val_loss: 0.0113\n",
            "Epoch 13/50\n",
            "\u001b[1m294/294\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.0344 - val_loss: 0.0136\n",
            "Epoch 14/50\n",
            "\u001b[1m294/294\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.0134 - val_loss: 0.0119\n",
            "Epoch 15/50\n",
            "\u001b[1m294/294\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.0116 - val_loss: 0.0103\n",
            "Epoch 16/50\n",
            "\u001b[1m294/294\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - loss: 0.0100 - val_loss: 0.0090\n",
            "Epoch 17/50\n",
            "\u001b[1m294/294\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.0094 - val_loss: 0.0087\n",
            "Epoch 18/50\n",
            "\u001b[1m294/294\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 0.0085 - val_loss: 0.0080\n",
            "Epoch 19/50\n",
            "\u001b[1m294/294\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - loss: 0.0082 - val_loss: 0.0082\n",
            "Epoch 20/50\n",
            "\u001b[1m294/294\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.0079 - val_loss: 0.0076\n",
            "Epoch 21/50\n",
            "\u001b[1m294/294\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0075 - val_loss: 0.0075\n",
            "Epoch 22/50\n",
            "\u001b[1m294/294\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.0082 - val_loss: 0.0072\n",
            "Epoch 23/50\n",
            "\u001b[1m294/294\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - loss: 0.0099 - val_loss: 0.0066\n",
            "Epoch 24/50\n",
            "\u001b[1m294/294\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - loss: 0.0121 - val_loss: 0.0068\n",
            "Epoch 25/50\n",
            "\u001b[1m294/294\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - loss: 0.0113 - val_loss: 0.0061\n",
            "Epoch 26/50\n",
            "\u001b[1m294/294\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - loss: 0.0069 - val_loss: 0.0057\n",
            "Epoch 27/50\n",
            "\u001b[1m294/294\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.0061 - val_loss: 0.0057\n",
            "Epoch 28/50\n",
            "\u001b[1m294/294\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0059 - val_loss: 0.0058\n",
            "Epoch 29/50\n",
            "\u001b[1m294/294\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.0059 - val_loss: 0.0057\n",
            "Epoch 30/50\n",
            "\u001b[1m294/294\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - loss: 0.0057 - val_loss: 0.0055\n",
            "Epoch 31/50\n",
            "\u001b[1m294/294\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 0.0060 - val_loss: 0.0056\n",
            "Epoch 32/50\n",
            "\u001b[1m294/294\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - loss: 0.0094 - val_loss: 0.0056\n",
            "Epoch 33/50\n",
            "\u001b[1m294/294\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0106 - val_loss: 0.0061\n",
            "Epoch 34/50\n",
            "\u001b[1m294/294\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.0102 - val_loss: 0.0050\n",
            "Epoch 35/50\n",
            "\u001b[1m294/294\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.0051 - val_loss: 0.0045\n",
            "Epoch 36/50\n",
            "\u001b[1m294/294\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - loss: 0.0050 - val_loss: 0.0045\n",
            "Epoch 37/50\n",
            "\u001b[1m294/294\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.0046 - val_loss: 0.0043\n",
            "Epoch 38/50\n",
            "\u001b[1m294/294\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.0049 - val_loss: 0.0045\n",
            "Epoch 39/50\n",
            "\u001b[1m294/294\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - loss: 0.0044 - val_loss: 0.0043\n",
            "Epoch 40/50\n",
            "\u001b[1m294/294\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.0043 - val_loss: 0.0041\n",
            "Epoch 41/50\n",
            "\u001b[1m294/294\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.0041 - val_loss: 0.0042\n",
            "Epoch 42/50\n",
            "\u001b[1m294/294\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.0043 - val_loss: 0.0041\n",
            "Epoch 43/50\n",
            "\u001b[1m294/294\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - loss: 0.0048 - val_loss: 0.0040\n",
            "Epoch 44/50\n",
            "\u001b[1m294/294\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - loss: 0.0102 - val_loss: 0.0051\n",
            "Epoch 45/50\n",
            "\u001b[1m294/294\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.0127 - val_loss: 0.0046\n",
            "Epoch 46/50\n",
            "\u001b[1m294/294\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - loss: 0.0092 - val_loss: 0.0043\n",
            "Epoch 47/50\n",
            "\u001b[1m294/294\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - loss: 0.0062 - val_loss: 0.0040\n",
            "Epoch 48/50\n",
            "\u001b[1m294/294\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0041 - val_loss: 0.0038\n",
            "Epoch 49/50\n",
            "\u001b[1m294/294\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.0039 - val_loss: 0.0036\n",
            "Epoch 50/50\n",
            "\u001b[1m294/294\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - loss: 0.0037 - val_loss: 0.0037\n",
            "Restoring model weights from the end of the best epoch: 49.\n",
            "Latent shapes: (167100, 12) (41776, 12)\n",
            "XGB train/val shapes: (150390, 12) (16710, 12)\n",
            "Training XGBoost with early stopping (xgb.train)...\n",
            "[0]\ttrain-logloss:0.64701\tval-logloss:0.64723\n",
            "[50]\ttrain-logloss:0.06362\tval-logloss:0.06741\n",
            "[100]\ttrain-logloss:0.02407\tval-logloss:0.02975\n",
            "[150]\ttrain-logloss:0.01828\tval-logloss:0.02525\n",
            "[200]\ttrain-logloss:0.01570\tval-logloss:0.02395\n",
            "[250]\ttrain-logloss:0.01405\tval-logloss:0.02342\n",
            "[300]\ttrain-logloss:0.01279\tval-logloss:0.02314\n",
            "[350]\ttrain-logloss:0.01181\tval-logloss:0.02307\n",
            "[395]\ttrain-logloss:0.01097\tval-logloss:0.02309\n",
            "\n",
            "📊 Final Evaluation (Autoencoder + XGB)\n",
            "Accuracy  : 0.995212562236691\n",
            "Precision : 0.9952278353406536\n",
            "Recall    : 0.995212562236691\n",
            "F1-score  : 0.9952125253245905\n",
            "Confusion matrix:\n",
            " [[20846    42]\n",
            " [  158 20730]]\n"
          ]
        }
      ]
    }
  ]
}