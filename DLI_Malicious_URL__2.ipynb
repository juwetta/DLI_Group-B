{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/juwetta/DLI_Group-B/blob/main/DLI_Malicious_URL__2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Mount Google Drive"
      ],
      "metadata": {
        "id": "Oa8V59xn94Xf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ODgUzXBv6dqr",
        "outputId": "01b1561a-6d84-4563-b9fc-597514464cef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import Libraries"
      ],
      "metadata": {
        "id": "829bwlSbRrsD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np, pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report, f1_score, confusion_matrix\n",
        "import xgboost as xgb"
      ],
      "metadata": {
        "id": "QlfGIR-sRuys"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# set paths"
      ],
      "metadata": {
        "id": "b6fnNHKw9-0x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Paths\n",
        "SRC = '/content/drive/MyDrive/DLI Group B/url_dataset/URL dataset.csv'\n",
        "OUT_DIR = '/content/drive/MyDrive/DLI Group B/url_dataset/clean_balanced'\n",
        "FINAL_CSV = f'{OUT_DIR}/URL_dataset_clean_balanced.csv'\n",
        "\n",
        "# Create output folder\n",
        "import os\n",
        "os.makedirs(OUT_DIR, exist_ok=True)\n"
      ],
      "metadata": {
        "id": "ge0rlt6b-HeB"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1) Load, clean, and deduplicate"
      ],
      "metadata": {
        "id": "OKTKnWGp-hSB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load\n",
        "df = pd.read_csv(SRC)\n",
        "\n",
        "#Convert datatype\n",
        "X_text = df['url'].astype(str)\n",
        "y      = df['type']\n",
        "\n",
        "# Clean: drop missing & exact duplicates\n",
        "df = df.dropna(subset=['url','type']).drop_duplicates().reset_index(drop=True)\n",
        "\n",
        "# Show cleaned stats\n",
        "print('After cleaning:', df.shape)\n",
        "print('Class counts (cleaned):\\n', df['type'].value_counts())\n",
        "\n",
        "# Find minority count (phishing ~104k in your data)\n",
        "minority_count = df['type'].value_counts().min()\n",
        "\n",
        "# Undersample each class to the minority count (no synthetic data)\n",
        "df_balanced = (\n",
        "    df.groupby('type', group_keys=False)\n",
        "      .apply(lambda x: x.sample(n=minority_count, random_state=42))\n",
        "      .sample(frac=1, random_state=42)   # shuffle\n",
        "      .reset_index(drop=True)\n",
        ")\n",
        "\n",
        "# Sanity checks\n",
        "print('\\nBalanced counts:\\n', df_balanced['type'].value_counts())\n",
        "print('Missing values:\\n', df_balanced.isnull().sum())\n",
        "print('Duplicate rows:', df_balanced.duplicated().sum())\n",
        "\n",
        "# Save final balanced & clean file\n",
        "df_balanced.to_csv(FINAL_CSV, index=False)\n",
        "print('\\nSaved ->', FINAL_CSV)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 370
        },
        "id": "94kjc3hsADe_",
        "outputId": "beb9aa83-4e16-487d-93db-20328b30aa62"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/drive/MyDrive/DLI Group B/url_dataset/URL dataset.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-412539766.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Load\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSRC\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m#Convert datatype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/DLI Group B/url_dataset/URL dataset.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Make dataset memory friendly"
      ],
      "metadata": {
        "id": "5OIP4h44R78j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Temp] loading data from github\n"
      ],
      "metadata": {
        "id": "7s7DHROiPpcF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DATA_URL = \"https://raw.githubusercontent.com/juwetta/DLI_Group-B/main/URL_dataset_clean_balanced.csv\"\n",
        "!wget -O URL_dataset_clean_balanced.csv \"$DATA_URL\"\n",
        "\n",
        "df = pd.read_csv(\"URL_dataset_clean_balanced.csv\")\n",
        "\n",
        "X_text = df['url'].astype(str)\n",
        "y      = df['type']           # strings: \"phishing\", \"legitimate"
      ],
      "metadata": {
        "id": "L7sUwwAfPvre",
        "outputId": "fab98246-2cf9-41a8-f102-42b26444a735",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-08-23 14:07:35--  https://raw.githubusercontent.com/juwetta/DLI_Group-B/main/URL_dataset_clean_balanced.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.110.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 15312637 (15M) [text/plain]\n",
            "Saving to: ‘URL_dataset_clean_balanced.csv’\n",
            "\n",
            "\r          URL_datas   0%[                    ]       0  --.-KB/s               \rURL_dataset_clean_b 100%[===================>]  14.60M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2025-08-23 14:07:36 (123 MB/s) - ‘URL_dataset_clean_balanced.csv’ saved [15312637/15312637]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2) Stratified train/val/test splits\n",
        "\n"
      ],
      "metadata": {
        "id": "3-BxToIeAfxs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train_text, X_test_text, y_train, y_test = train_test_split(\n",
        "    X_text, y, test_size=0.2, stratify=y, random_state=42\n",
        ")\n",
        "print(len(X_train_text), len(X_test_text), y_train.value_counts().to_dict())\n",
        "\n",
        "# train_df, temp_df = train_test_split(\n",
        "#     df_balanced, test_size=0.30, stratify=df_balanced['type'], random_state=42\n",
        "# )\n",
        "# val_df, test_df = train_test_split(\n",
        "#     temp_df, test_size=0.50, stratify=temp_df['type'], random_state=42\n",
        "# )\n",
        "\n",
        "# print('Split sizes -> train/val/test:', len(train_df), len(val_df), len(test_df))\n",
        "# print('Train counts:\\n', train_df['type'].value_counts())\n",
        "# print('Val counts:\\n',   val_df['type'].value_counts())\n",
        "# print('Test counts:\\n',  test_df['type'].value_counts())\n",
        "\n",
        "# train_df.to_csv(f'{OUT_DIR}/URL_dataset_balanced_train.csv', index=False)\n",
        "# val_df.to_csv(f'{OUT_DIR}/URL_dataset_balanced_val.csv', index=False)\n",
        "# test_df.to_csv(f'{OUT_DIR}/URL_dataset_balanced_test.csv', index=False)\n",
        "# print('\\nSaved splits to:', OUT_DIR)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mT0MoI6aViaj",
        "outputId": "e82351d8-5c35-4693-a348-c1ba682a3e37"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "167100 41776 {'legitimate': 83550, 'phishing': 83550}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Make dataset memory friendly"
      ],
      "metadata": {
        "id": "hpsWmMbKScir"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Smaller TF-IDF to avoid RAM issues\n",
        "tfidf = TfidfVectorizer(\n",
        "    analyzer='char',\n",
        "    ngram_range=(3,4),\n",
        "    min_df=5,\n",
        "    max_features=10000,\n",
        "    dtype=np.float32\n",
        ")\n",
        "\n",
        "X_train_tfidf = tfidf.fit_transform(X_train_text)\n",
        "X_test_tfidf  = tfidf.transform(X_test_text)\n",
        "\n",
        "# Reduce to 200 dims (dense but tiny)\n",
        "svd = TruncatedSVD(n_components=200, random_state=42)\n",
        "X_train_200 = svd.fit_transform(X_train_tfidf)\n",
        "X_test_200  = svd.transform(X_test_tfidf)\n",
        "\n",
        "print(\"Shapes ->\", X_train_200.shape, X_test_200.shape)"
      ],
      "metadata": {
        "id": "tQ-cHXIgSWR4",
        "outputId": "ec339862-f106-46ae-c3bb-0689331e1e13",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shapes -> (167100, 200) (41776, 200)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3) Train Model: XGBoost (hist) on 200-D"
      ],
      "metadata": {
        "id": "rDXRDoYKQ3Wg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.1) Label Encoding"
      ],
      "metadata": {
        "id": "iz82pmvDRFXz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# y_train / y_test are strings -> encode to 0/1\n",
        "le = LabelEncoder()\n",
        "y_train_enc = le.fit_transform(y_train)   # e.g., legitimate->0, phishing->1\n",
        "y_test_enc  = le.transform(y_test)\n",
        "\n",
        "print(\"Classes mapping:\", dict(zip(le.classes_, le.transform(le.classes_))))"
      ],
      "metadata": {
        "id": "rMuKXk6VMpf1",
        "outputId": "782397ef-fc54-4bd0-bdf7-20a6624b4d37",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classes mapping: {'legitimate': np.int64(0), 'phishing': np.int64(1)}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ---- XGBoost fit ----\n",
        "xgb_model = xgb.XGBClassifier(\n",
        "    n_estimators=250,\n",
        "    learning_rate=0.08,\n",
        "    max_depth=7,\n",
        "    subsample=0.9,\n",
        "    colsample_bytree=0.9,\n",
        "    tree_method='hist',\n",
        "    random_state=42,\n",
        "    eval_metric='logloss'\n",
        ")\n",
        "\n",
        "xgb_model.fit(X_train_200, y_train_enc)\n",
        "\n",
        "# Default-threshold predictions (0/1)\n",
        "y_pred_enc = xgb_model.predict(X_test_200)\n",
        "\n",
        "# OPTIONAL: map back to strings for reports\n",
        "y_pred = le.inverse_transform(y_pred_enc)"
      ],
      "metadata": {
        "id": "ViXP3Y6KRQBu"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4) Evaluate Model"
      ],
      "metadata": {
        "id": "NasVfiCNTH5j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "proba = xgb_model.predict_proba(X_test_200)[:, 1]   # P(class=1)\n",
        "phish_id = le.transform(['phishing'])[0]\n",
        "\n",
        "best_f1, best_t = 0, 0.5\n",
        "for t in np.arange(0.30, 0.71, 0.01):\n",
        "    y_pred_t = (proba >= t).astype(int)\n",
        "    f1 = f1_score(y_test_enc, y_pred_t, pos_label=phish_id)\n",
        "    if f1 > best_f1:\n",
        "        best_f1, best_t = f1, t\n",
        "\n",
        "print(f\"Best F1={best_f1:.4f} at threshold={best_t:.2f}\")\n",
        "\n",
        "# If you want the string labels for that best threshold:\n",
        "y_pred_best = le.inverse_transform((proba >= best_t).astype(int))\n",
        "print(classification_report(y_test, y_pred_best, digits=3))"
      ],
      "metadata": {
        "id": "2bkQJ0cbTbhY",
        "outputId": "123bc48c-24bb-446d-a668-18bd713f3538",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best F1=0.9966 at threshold=0.41\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "  legitimate      0.994     0.999     0.997     20888\n",
            "    phishing      0.999     0.994     0.997     20888\n",
            "\n",
            "    accuracy                          0.997     41776\n",
            "   macro avg      0.997     0.997     0.997     41776\n",
            "weighted avg      0.997     0.997     0.997     41776\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jjSBVnk3TeG-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}