{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOhBYm1iO7461Jjb00vqphJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/juwetta/DLI_Group-B/blob/main/DLI_Malicious_URL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L0TmHpBRdt1V",
        "outputId": "b660d3ec-cb4f-4cbb-dd98-ad0ba367c07c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import imporant libraries"
      ],
      "metadata": {
        "id": "W661yBy9oQiC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from sklearn.datasets import load_svmlight_file\n",
        "import glob\n",
        "import scipy.sparse\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "2rPIABucoPoQ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "[code below is used for lower the size of dataset, since the full dataset can used up all the RAM just to storing them]"
      ],
      "metadata": {
        "id": "tBTgFy_JS4WI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import shutil\n",
        "\n",
        "# # Define the source folder path\n",
        "# source_folder = '/content/drive/My Drive/DLI Group B/url_svmlight'\n",
        "\n",
        "# # Define the destination folder path\n",
        "# destination_folder = '/content/drive/My Drive/DLI Group B/url_dataset'\n",
        "\n",
        "# # Create the destination folder if it doesn't exist\n",
        "# os.makedirs(destination_folder, exist_ok=True)\n",
        "\n",
        "# # List of files to copy (Day0.svm to Day60.svm and FeatureTypes)\n",
        "# files_to_copy = [f'Day{i}.svm' for i in range(61)] + ['FeatureTypes']\n",
        "\n",
        "# print(f\"Copying files to: {destination_folder}\")\n",
        "\n",
        "# # Copy the specified files\n",
        "# for file_name in files_to_copy:\n",
        "#     source_file_path = os.path.join(source_folder, file_name)\n",
        "#     destination_file_path = os.path.join(destination_folder, file_name)\n",
        "#     try:\n",
        "#         shutil.copy2(source_file_path, destination_file_path)\n",
        "#         print(f\"Copied: {file_name}\")\n",
        "#     except FileNotFoundError:\n",
        "#         print(f\"Warning: File not found - {file_name}. Skipping.\")\n",
        "#     except Exception as e:\n",
        "#         print(f\"Error copying file {file_name}: {e}\")\n",
        "\n",
        "# print(\"File copying process completed.\")"
      ],
      "metadata": {
        "id": "lU_Rvf6xR39n"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Accessing to folder"
      ],
      "metadata": {
        "id": "AzFwtL3ZoWsH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "folder_path = '/content/drive/My Drive/DLI Group B/url_dataset'\n",
        "\n",
        "svm_files = glob.glob(os.path.join(folder_path, \"*.svm\"))\n",
        "print(f\"Found {len(svm_files)} SVM files in: {folder_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cLsnaAxMobYW",
        "outputId": "3d0633d7-e6aa-4ca2-ab68-714fdd6213bf"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 21 SVM files in: /content/drive/My Drive/DLI Group B/url_dataset\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the context of the output, \"shape\" refers to the dimensions of the data structure (like a NumPy array or a SciPy sparse matrix).\n",
        "\n",
        "For a 2D structure like combined_X, the shape (2396130, 3231961) means it has 2,396,130 rows and 3,231,961 columns. In this dataset, the rows represent the samples (e.g., URLs), and the columns represent the features.\n",
        "For a 1D structure like combined_y, the shape (2396130,) means it has 2,396,130 elements. This corresponds to the labels for each of the samples in combined_X.\n",
        "So, the shape tells you how many samples you have and how many features or labels are associated with each sample."
      ],
      "metadata": {
        "id": "EPR2aZsXJWN2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Read SVM file and store dataset\n"
      ],
      "metadata": {
        "id": "MKkrToETooYg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "the different SVM files have varying numbers of features, causing an error when trying to combine them. I'll add a step to determine the total number of features across all files and then load each file with that consistent number of features."
      ],
      "metadata": {
        "id": "G1tzLDporKmY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# max_features = 0\n",
        "\n",
        "# for file_path in svm_files:\n",
        "#   try:\n",
        "#     X, _ = load_svmlight_file(file_path)\n",
        "#     if X.shape[1] > max_features:\n",
        "#       max_features = X.shape[1]\n",
        "\n",
        "#   except Exception as e:\n",
        "#     print(f\"Error loading file {os.path.basename(file_path)}: {e}\")\n",
        "\n",
        "\n",
        "# print(f\"Maximum number of features found: {max_features}\") #74.777s used\n",
        "max_features = 3231961\n",
        "print(f\"Maximum number of features found: {max_features}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-v3O3-CsqtRQ",
        "outputId": "46014556-1844-4906-b996-87e9edec89f9"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Maximum number of features found: 3231961\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "all_X = []\n",
        "all_y = []\n",
        "max_features = 3231961\n",
        "try:\n",
        "    print(\"\\nLoading and combining data...\")\n",
        "    counter = 0\n",
        "    # Load and combine data in a single pass, specifying the number of features\n",
        "    for file_path in svm_files:\n",
        "        try:\n",
        "            if counter == 3: break\n",
        "            X, y = load_svmlight_file(file_path, n_features=max_features)\n",
        "            all_X.append(X)\n",
        "            all_y.append(y)\n",
        "            print(f\"{os.path.basename(file_path)}\", end=\"| \")\n",
        "            counter += 1\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading file {os.path.basename(file_path)}: {e}\")\n",
        "\n",
        "    if all_X and all_y:\n",
        "        # Vertically stack the sparse feature matrices\n",
        "        combined_X = scipy.sparse.vstack(all_X)\n",
        "        # Concatenate the label arrays\n",
        "        combined_y = np.concatenate(all_y)\n",
        "\n",
        "        print(\"\\nSuccessfully combined data from all files.\")\n",
        "        print(f\"Shape of combined data (X): {combined_X.shape}\")\n",
        "        print(f\"Shape of combined labels (y): {combined_y.shape}\")\n",
        "    else:\n",
        "        print(\"\\nNo data was loaded from the SVM files.\")\n",
        "\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"Folder not found at: {folder_path}\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\") #75.54s used"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AIVvCQxf9EAH",
        "outputId": "c113fef8-e83f-4a6b-f8f4-b3284e6053d0"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Loading and combining data...\n",
            "Day0.svm| Day1.svm| Day2.svm| \n",
            "Successfully combined data from all files.\n",
            "Shape of combined data (X): (56000, 3231961)\n",
            "Shape of combined labels (y): (56000,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Identify the indexes of real-valued features"
      ],
      "metadata": {
        "id": "_YDHTWMvpDjk"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ad19fa23",
        "outputId": "3ed95051-021b-4eb4-b17c-0d29f8a277b5"
      },
      "source": [
        "import os\n",
        "\n",
        "feature_types_path = '/content/drive/My Drive/DLI Group B/url_dataset/FeatureTypes'\n",
        "real_valued_feature_indices = set()\n",
        "\n",
        "try:\n",
        "    with open(feature_types_path, 'r') as f:\n",
        "        for line in f:\n",
        "            # Assuming each line in FeatureTypes is a feature index\n",
        "            try:\n",
        "                index = int(line.strip())\n",
        "                real_valued_feature_indices.add(index)\n",
        "            except ValueError:\n",
        "                # Handle potential non-integer lines in the file\n",
        "                print(f\"Skipping non-integer line in FeatureTypes: {line.strip()}\")\n",
        "\n",
        "    print(f\"Identified {len(real_valued_feature_indices)} real-valued feature indices.\")\n",
        "    # print(\"First 10 real-valued feature indices:\", list(real_valued_feature_indices)[:10]) # Optional: print a few indices\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"FeatureTypes file not found at: {feature_types_path}\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred while reading FeatureTypes: {e}\")\n",
        "\n",
        "# Now you have the set of real-valued feature indices and can use it\n",
        "# For example, you could filter your data or analyze these specific features."
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Identified 64 real-valued feature indices.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Briefly explore the dataset"
      ],
      "metadata": {
        "id": "3d8LW7gkpK-3"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2e46d2c3",
        "outputId": "903498b3-dbbb-4eef-b246-256c6da32323"
      },
      "source": [
        "# Select the first few rows to inspect\n",
        "num_rows_to_inspect = 5\n",
        "sample_rows = combined_X[:num_rows_to_inspect]\n",
        "\n",
        "print(f\"Values of real-valued features in the first {num_rows_to_inspect} rows:\")\n",
        "\n",
        "# Iterate through the selected rows\n",
        "for i in range(sample_rows.shape[0]):\n",
        "    print(f\"\\nRow {i+1}:\")\n",
        "    # Iterate through the real-valued feature indices\n",
        "    for feature_index in sorted(list(real_valued_feature_indices)): # Sorting for consistent output\n",
        "        # Check if the feature exists in the current row (i.e., it's non-zero)\n",
        "\n",
        "        if feature_index in sample_rows[i].indices:\n",
        "            # Get the index within the non-zero elements\n",
        "            data_index = np.where(sample_rows[i].indices == feature_index)[0][0]\n",
        "            # Get the value of the feature\n",
        "            feature_value = sample_rows[i].data[data_index]\n",
        "            print(f\"  Feature {feature_index}: {feature_value}\")\n",
        "        # If the feature index is not in sample_rows[i].indices, its value is 0 in the sparse matrix,\n",
        "        # so we don't need to explicitly print 0 unless we want to see all real-valued features\n",
        "        # even if their value is 0 for that sample. Let's only print non-zero real-valued features."
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Values of real-valued features in the first 5 rows:\n",
            "\n",
            "Row 1:\n",
            "  Feature 4: 0.124138\n",
            "  Feature 5: 0.117647\n",
            "  Feature 16: 0.749633\n",
            "  Feature 17: 0.843029\n",
            "  Feature 18: 0.197344\n",
            "  Feature 21: 0.142857\n",
            "  Feature 22: 0.142857\n",
            "  Feature 55: 1.0\n",
            "  Feature 63: 1.0\n",
            "  Feature 69: 1.0\n",
            "  Feature 71: 1.0\n",
            "  Feature 73: 1.0\n",
            "  Feature 75: 1.0\n",
            "  Feature 81: 1.0\n",
            "  Feature 83: 1.0\n",
            "  Feature 85: 1.0\n",
            "  Feature 87: 1.0\n",
            "  Feature 89: 1.0\n",
            "  Feature 91: 1.0\n",
            "  Feature 93: 1.0\n",
            "  Feature 95: 1.0\n",
            "  Feature 101: 1.0\n",
            "  Feature 103: 1.0\n",
            "  Feature 105: 1.0\n",
            "  Feature 107: 1.0\n",
            "  Feature 109: 1.0\n",
            "  Feature 111: 1.0\n",
            "\n",
            "Row 2:\n",
            "  Feature 4: 0.103448\n",
            "  Feature 5: 0.176471\n",
            "  Feature 16: 0.72266\n",
            "  Feature 17: 0.836498\n",
            "  Feature 18: 0.6189\n",
            "  Feature 21: 0.0119048\n",
            "  Feature 23: 1.0\n",
            "  Feature 55: 1.0\n",
            "  Feature 61: 1.0\n",
            "  Feature 63: 1.0\n",
            "  Feature 65: 1.0\n",
            "  Feature 67: 1.0\n",
            "  Feature 69: 1.0\n",
            "  Feature 71: 1.0\n",
            "  Feature 73: 1.0\n",
            "  Feature 75: 1.0\n",
            "  Feature 81: 1.0\n",
            "  Feature 83: 1.0\n",
            "  Feature 85: 1.0\n",
            "  Feature 87: 1.0\n",
            "  Feature 89: 1.0\n",
            "  Feature 91: 1.0\n",
            "  Feature 101: 1.0\n",
            "  Feature 103: 1.0\n",
            "  Feature 105: 1.0\n",
            "  Feature 107: 1.0\n",
            "  Feature 109: 1.0\n",
            "  Feature 111: 1.0\n",
            "  Feature 126: 1.0\n",
            "  Feature 150: 1.0\n",
            "\n",
            "Row 3:\n",
            "  Feature 4: 0.144828\n",
            "  Feature 5: 0.117647\n",
            "  Feature 16: 0.760482\n",
            "  Feature 17: 0.820882\n",
            "  Feature 18: 0.150678\n",
            "  Feature 21: 0.142857\n",
            "  Feature 23: 1.0\n",
            "  Feature 43: 1.0\n",
            "  Feature 55: 1.0\n",
            "  Feature 61: 1.0\n",
            "  Feature 63: 1.0\n",
            "  Feature 65: 1.0\n",
            "  Feature 67: 1.0\n",
            "  Feature 69: 1.0\n",
            "  Feature 71: 1.0\n",
            "  Feature 73: 1.0\n",
            "  Feature 75: 1.0\n",
            "  Feature 81: 1.0\n",
            "  Feature 83: 1.0\n",
            "  Feature 85: 1.0\n",
            "  Feature 87: 1.0\n",
            "  Feature 89: 1.0\n",
            "  Feature 91: 1.0\n",
            "  Feature 93: 1.0\n",
            "  Feature 95: 1.0\n",
            "  Feature 101: 1.0\n",
            "  Feature 103: 1.0\n",
            "  Feature 105: 1.0\n",
            "  Feature 107: 1.0\n",
            "  Feature 109: 1.0\n",
            "  Feature 111: 1.0\n",
            "\n",
            "Row 4:\n",
            "  Feature 4: 0.0896552\n",
            "  Feature 5: 0.176471\n",
            "  Feature 21: 0.0238095\n",
            "  Feature 23: 1.0\n",
            "  Feature 35: 1.0\n",
            "  Feature 43: 1.0\n",
            "  Feature 55: 1.0\n",
            "  Feature 61: 1.0\n",
            "  Feature 63: 1.0\n",
            "  Feature 65: 1.0\n",
            "  Feature 67: 1.0\n",
            "  Feature 69: 1.0\n",
            "  Feature 71: 1.0\n",
            "  Feature 73: 1.0\n",
            "  Feature 75: 1.0\n",
            "  Feature 81: 1.0\n",
            "  Feature 83: 1.0\n",
            "  Feature 85: 1.0\n",
            "  Feature 87: 1.0\n",
            "  Feature 89: 1.0\n",
            "  Feature 91: 1.0\n",
            "  Feature 93: 1.0\n",
            "  Feature 95: 1.0\n",
            "  Feature 101: 1.0\n",
            "  Feature 103: 1.0\n",
            "  Feature 105: 1.0\n",
            "  Feature 107: 1.0\n",
            "  Feature 109: 1.0\n",
            "  Feature 111: 1.0\n",
            "  Feature 150: 1.0\n",
            "\n",
            "Row 5:\n",
            "  Feature 4: 0.131034\n",
            "  Feature 5: 0.117647\n",
            "  Feature 16: 0.830283\n",
            "  Feature 17: 0.83965\n",
            "  Feature 18: 0.583194\n",
            "  Feature 19: 1.0\n",
            "  Feature 21: 0.00595238\n",
            "  Feature 22: 0.00595238\n",
            "  Feature 35: 1.0\n",
            "  Feature 43: 1.0\n",
            "  Feature 55: 1.0\n",
            "  Feature 61: 1.0\n",
            "  Feature 63: 1.0\n",
            "  Feature 65: 1.0\n",
            "  Feature 67: 1.0\n",
            "  Feature 69: 1.0\n",
            "  Feature 71: 1.0\n",
            "  Feature 73: 1.0\n",
            "  Feature 75: 1.0\n",
            "  Feature 81: 1.0\n",
            "  Feature 83: 1.0\n",
            "  Feature 85: 1.0\n",
            "  Feature 87: 1.0\n",
            "  Feature 89: 1.0\n",
            "  Feature 91: 1.0\n",
            "  Feature 93: 1.0\n",
            "  Feature 95: 1.0\n",
            "  Feature 101: 1.0\n",
            "  Feature 103: 1.0\n",
            "  Feature 105: 1.0\n",
            "  Feature 107: 1.0\n",
            "  Feature 109: 1.0\n",
            "  Feature 111: 1.0\n",
            "  Feature 132: 1.0\n",
            "  Feature 138: 1.0\n",
            "  Feature 140: 1.0\n",
            "  Feature 142: 1.0\n",
            "  Feature 144: 1.0\n",
            "  Feature 146: 1.0\n",
            "  Feature 148: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eb21fdfb"
      },
      "source": [
        "# Task\n",
        "Balance the classes in the `combined_y` variable of the dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e12ae549"
      },
      "source": [
        "## Check class distribution\n",
        "\n",
        "### Subtask:\n",
        "Analyze the current distribution of the target variable (`combined_y`) to see how imbalanced the classes are.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aed82c52"
      },
      "source": [
        "**Reasoning**:\n",
        "Calculate and print the counts of each unique class in the `combined_y` array to understand the class distribution.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5bb1f3a4",
        "outputId": "255a8198-b5f7-4fda-ae3f-782a6c5aaf5a"
      },
      "source": [
        "unique_classes, class_counts = np.unique(combined_y, return_counts=True)\n",
        "\n",
        "print(\"Class distribution in combined_y:\")\n",
        "for class_val, count in zip(unique_classes, class_counts):\n",
        "    print(f\"Class {int(class_val)}: {count}\")"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class distribution in combined_y:\n",
            "Class -1: 35563\n",
            "Class 1: 20437\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "50ad112b"
      },
      "source": [
        "## Choose a balancing technique\n",
        "\n",
        "### Subtask:\n",
        "Decide on an appropriate method for balancing the classes, such as oversampling (e.g., SMOTE), undersampling, or a combination.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "562c76ef"
      },
      "source": [
        "## Apply the balancing technique\n",
        "\n",
        "### Subtask:\n",
        "Apply the balancing technique\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dadc4dfd"
      },
      "source": [
        "**Reasoning**:\n",
        "To balance the dataset, I will first import the necessary libraries, `SMOTE` and `RandomUnderSampler`. Then, I will apply SMOTE to oversample the minority class, followed by RandomUnderSampler to undersample the majority class, as per the instructions. This will create a more balanced dataset for model training.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "24022c39",
        "outputId": "c8e82d0d-9b26-4b02-9395-111da52cbbcf"
      },
      "source": [
        "from imblearn.under_sampling import NearMiss\n",
        "\n",
        "# Instantiate NearMiss\n",
        "# NearMiss has different versions (1, 2, 3). You can experiment with them.\n",
        "# NearMiss-1 selects samples from the majority class whose average distance to the k nearest samples of the minority class is smallest.\n",
        "# NearMiss-2 selects samples from the majority class whose average distance to the k farthest samples of the minority class is smallest.\n",
        "# NearMiss-3 selects samples from the majority class whose distance to the k nearest samples of the minority class is largest.\n",
        "# Let's start with NearMiss-1.\n",
        "nm = NearMiss(version=1, sampling_strategy='auto', n_neighbors=1) # Adjust n_neighbors if needed\n",
        "\n",
        "print(\"Applying NearMiss undersampling...\")\n",
        "# temp_combined_X, temp_combined_y = combined_X, combined_y\n",
        "# Apply NearMiss to the data\n",
        "# NearMiss works directly on the original data\n",
        "X_resampled, y_resampled = nm.fit_resample(temp_combined_X, temp_combined_X)\n",
        "\n",
        "# Print the shapes of the resampled data to verify\n",
        "print(f\"Shape of X after NearMiss: {X_resampled.shape}\")\n",
        "print(f\"Shape of y after NearMiss: {y_resampled.shape}\")\n",
        "\n",
        "# Verify the new class distribution\n",
        "unique_classes_resampled, class_counts_resampled = np.unique(y_resampled, return_counts=True)\n",
        "print(\"\\nClass distribution after resampling:\")\n",
        "for class_val, count in zip(unique_classes_resampled, class_counts_resampled):\n",
        "    print(f\"Class {int(class_val)}: {count}\")"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Applying NearMiss undersampling...\n",
            "Shape of X after NearMiss: (40874, 3231961)\n",
            "Shape of y after NearMiss: (40874,)\n",
            "\n",
            "Class distribution after resampling:\n",
            "Class -1: 20437\n",
            "Class 1: 20437\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "56a5aa52"
      },
      "source": [
        "## Train a classification model\n",
        "\n",
        "Train a classification model on the balanced dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "323fa674",
        "outputId": "11dc705c-3136-484f-c0a4-4e65237df761"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "\n",
        "# Split the resampled data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.3, random_state=42, stratify=y_resampled)\n",
        "\n",
        "print(\"Training Logistic Regression model...\")\n",
        "\n",
        "# Initialize and train the Logistic Regression model\n",
        "# Due to the large number of features, we might need to adjust solver and max_iter\n",
        "model = LogisticRegression(solver='liblinear', max_iter=1000, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "print(\"Model training complete.\")\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "print(\"\\nModel Evaluation:\")\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Logistic Regression model...\n",
            "Model training complete.\n",
            "\n",
            "Model Evaluation:\n",
            "Accuracy: 0.9842615999347631\n",
            "\n",
            "Confusion Matrix:\n",
            " [[5980  152]\n",
            " [  41 6090]]\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "        -1.0       0.99      0.98      0.98      6132\n",
            "         1.0       0.98      0.99      0.98      6131\n",
            "\n",
            "    accuracy                           0.98     12263\n",
            "   macro avg       0.98      0.98      0.98     12263\n",
            "weighted avg       0.98      0.98      0.98     12263\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8d3756a9"
      },
      "source": [
        "## Model Evaluation\n",
        "\n",
        "Analyze the performance of the trained model using appropriate metrics for imbalanced datasets."
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0EKZewTjk0uJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1bce8ea8"
      },
      "source": [
        "## Train a Support Vector Machine Model\n",
        "\n",
        "Train a Linear Support Vector Classification model on the balanced dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "790dfecb",
        "outputId": "b309d60b-10f1-4c8d-def1-4215ab289d1c"
      },
      "source": [
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "\n",
        "print(\"Training Linear SVC model...\")\n",
        "\n",
        "# Initialize and train the LinearSVC model\n",
        "# Adjust max_iter if convergence warnings occur\n",
        "svm_model = LinearSVC(max_iter=1000, random_state=42)\n",
        "svm_model.fit(X_train, y_train)\n",
        "\n",
        "print(\"Model training complete.\")\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred_svm = svm_model.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "print(\"\\nLinear SVC Model Evaluation:\")\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred_svm))\n",
        "print(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_test, y_pred_svm))\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred_svm))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Linear SVC model...\n",
            "Model training complete.\n",
            "\n",
            "Linear SVC Model Evaluation:\n",
            "Accuracy: 0.9855663377640056\n",
            "\n",
            "Confusion Matrix:\n",
            " [[5988  144]\n",
            " [  33 6098]]\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "        -1.0       0.99      0.98      0.99      6132\n",
            "         1.0       0.98      0.99      0.99      6131\n",
            "\n",
            "    accuracy                           0.99     12263\n",
            "   macro avg       0.99      0.99      0.99     12263\n",
            "weighted avg       0.99      0.99      0.99     12263\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/svm/_base.py:1249: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "29737af0"
      },
      "source": [
        "## Linear SVC Model Evaluation\n",
        "\n",
        "Analyze the performance of the trained Linear SVC model."
      ]
    }
  ]
}