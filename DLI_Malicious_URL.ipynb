{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOG5fOaC2PI/8hTZ74SCYlO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/juwetta/DLI_Group-B/blob/main/DLI_Malicious_URL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L0TmHpBRdt1V",
        "outputId": "e7744d23-84e9-4066-aa0a-c6c2bcd1103f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import imporant libraries"
      ],
      "metadata": {
        "id": "W661yBy9oQiC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from sklearn.datasets import load_svmlight_file\n",
        "import glob\n",
        "import scipy.sparse\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "2rPIABucoPoQ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Accessing to folder"
      ],
      "metadata": {
        "id": "AzFwtL3ZoWsH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "folder_path = '/content/drive/My Drive/DLI Group B/url_svmlight'\n",
        "\n",
        "svm_files = glob.glob(os.path.join(folder_path, \"*.svm\"))\n",
        "print(f\"Found {len(svm_files)} SVM files in: {folder_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cLsnaAxMobYW",
        "outputId": "dc269dba-1895-494a-edc9-e40ec5eada2d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 121 SVM files in: /content/drive/My Drive/DLI Group B/url_svmlight\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the context of the output, \"shape\" refers to the dimensions of the data structure (like a NumPy array or a SciPy sparse matrix).\n",
        "\n",
        "For a 2D structure like combined_X, the shape (2396130, 3231961) means it has 2,396,130 rows and 3,231,961 columns. In this dataset, the rows represent the samples (e.g., URLs), and the columns represent the features.\n",
        "For a 1D structure like combined_y, the shape (2396130,) means it has 2,396,130 elements. This corresponds to the labels for each of the samples in combined_X.\n",
        "So, the shape tells you how many samples you have and how many features or labels are associated with each sample."
      ],
      "metadata": {
        "id": "EPR2aZsXJWN2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Read SVM file and store dataset\n"
      ],
      "metadata": {
        "id": "MKkrToETooYg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "the different SVM files have varying numbers of features, causing an error when trying to combine them. I'll add a step to determine the total number of features across all files and then load each file with that consistent number of features."
      ],
      "metadata": {
        "id": "G1tzLDporKmY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# max_features = 0\n",
        "\n",
        "# for file_path in svm_files:\n",
        "#   try:\n",
        "#     X, _ = load_svmlight_file(file_path)\n",
        "#     if X.shape[1] > max_features:\n",
        "#       max_features = X.shape[1]\n",
        "\n",
        "#   except Exception as e:\n",
        "#     print(f\"Error loading file {os.path.basename(file_path)}: {e}\")\n",
        "\n",
        "\n",
        "# print(f\"Maximum number of features found: {max_features}\") #146.758s used\n",
        "max_features = 3231961\n",
        "print(f\"Maximum number of features found: {max_features}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-v3O3-CsqtRQ",
        "outputId": "7f5a5b90-46cb-4216-b5e4-0e47851d3fdf"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Maximum number of features found: 3231961\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "all_X = []\n",
        "all_y = []\n",
        "max_features = 3231961\n",
        "try:\n",
        "    print(\"\\nLoading and combining data...\")\n",
        "\n",
        "    # Load and combine data in a single pass, specifying the number of features\n",
        "    for file_path in svm_files:\n",
        "        try:\n",
        "            X, y = load_svmlight_file(file_path, n_features=max_features)\n",
        "            all_X.append(X)\n",
        "            all_y.append(y)\n",
        "            print(f\"{os.path.basename(file_path)}\", end=\"| \")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading file {os.path.basename(file_path)}: {e}\")\n",
        "\n",
        "    if all_X and all_y:\n",
        "        # Vertically stack the sparse feature matrices\n",
        "        combined_X = scipy.sparse.vstack(all_X)\n",
        "        # Concatenate the label arrays\n",
        "        combined_y = np.concatenate(all_y)\n",
        "\n",
        "        print(\"\\nSuccessfully combined data from all files.\")\n",
        "        print(f\"Shape of combined data (X): {combined_X.shape}\")\n",
        "        print(f\"Shape of combined labels (y): {combined_y.shape}\")\n",
        "    else:\n",
        "        print(\"\\nNo data was loaded from the SVM files.\")\n",
        "\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"Folder not found at: {folder_path}\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\") #154.318s used"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AIVvCQxf9EAH",
        "outputId": "b9e9f45a-ef28-429d-a696-d316567fd31d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Loading and combining data...\n",
            "Day0.svm| Day1.svm| Day2.svm| Day3.svm| Day4.svm| Day5.svm| Day6.svm| Day7.svm| Day8.svm| Day9.svm| Day10.svm| Day11.svm| Day12.svm| Day13.svm| Day14.svm| Day15.svm| Day16.svm| Day17.svm| Day18.svm| Day19.svm| Day20.svm| Day21.svm| Day22.svm| Day23.svm| Day24.svm| Day25.svm| Day26.svm| Day27.svm| Day28.svm| Day29.svm| Day30.svm| Day31.svm| Day32.svm| Day33.svm| Day34.svm| Day35.svm| Day36.svm| Day37.svm| Day38.svm| Day39.svm| Day40.svm| Day41.svm| Day42.svm| Day43.svm| Day44.svm| Day45.svm| Day46.svm| Day47.svm| Day48.svm| Day49.svm| Day50.svm| Day51.svm| Day52.svm| Day53.svm| Day54.svm| Day55.svm| Day56.svm| Day57.svm| Day58.svm| Day59.svm| Day60.svm| Day61.svm| Day62.svm| Day63.svm| Day64.svm| Day65.svm| Day66.svm| Day67.svm| Day68.svm| Day69.svm| Day70.svm| Day71.svm| Day72.svm| Day73.svm| Day74.svm| Day75.svm| Day76.svm| Day77.svm| Day78.svm| Day79.svm| Day80.svm| Day81.svm| Day82.svm| Day83.svm| Day84.svm| Day85.svm| Day86.svm| Day87.svm| Day88.svm| Day89.svm| Day90.svm| Day91.svm| Day92.svm| Day93.svm| Day94.svm| Day95.svm| Day96.svm| Day97.svm| Day98.svm| Day99.svm| Day100.svm| Day101.svm| Day102.svm| Day103.svm| Day104.svm| Day105.svm| Day106.svm| Day107.svm| Day108.svm| Day109.svm| Day110.svm| Day111.svm| Day112.svm| Day113.svm| Day114.svm| Day115.svm| Day116.svm| Day117.svm| Day118.svm| Day119.svm| Day120.svm| \n",
            "Successfully combined data from all files.\n",
            "Shape of combined data (X): (2396130, 3231961)\n",
            "Shape of combined labels (y): (2396130,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Identify the indexes of real-valued features"
      ],
      "metadata": {
        "id": "_YDHTWMvpDjk"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ad19fa23",
        "outputId": "c308bf02-ee4c-41b3-eae9-c0c8acee1da3"
      },
      "source": [
        "import os\n",
        "\n",
        "feature_types_path = '/content/drive/My Drive/DLI Group B/url_svmlight/FeatureTypes'\n",
        "real_valued_feature_indices = set()\n",
        "\n",
        "try:\n",
        "    with open(feature_types_path, 'r') as f:\n",
        "        for line in f:\n",
        "            # Assuming each line in FeatureTypes is a feature index\n",
        "            try:\n",
        "                index = int(line.strip())\n",
        "                real_valued_feature_indices.add(index)\n",
        "            except ValueError:\n",
        "                # Handle potential non-integer lines in the file\n",
        "                print(f\"Skipping non-integer line in FeatureTypes: {line.strip()}\")\n",
        "\n",
        "    print(f\"Identified {len(real_valued_feature_indices)} real-valued feature indices.\")\n",
        "    # print(\"First 10 real-valued feature indices:\", list(real_valued_feature_indices)[:10]) # Optional: print a few indices\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"FeatureTypes file not found at: {feature_types_path}\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred while reading FeatureTypes: {e}\")\n",
        "\n",
        "# Now you have the set of real-valued feature indices and can use it\n",
        "# For example, you could filter your data or analyze these specific features."
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Identified 64 real-valued feature indices.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Briefly explore the dataset"
      ],
      "metadata": {
        "id": "3d8LW7gkpK-3"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2e46d2c3",
        "outputId": "ed2a3b0d-ed37-4e05-b0a0-875198e2f4d9"
      },
      "source": [
        "# Select the first few rows to inspect\n",
        "num_rows_to_inspect = 5\n",
        "sample_rows = combined_X[:num_rows_to_inspect]\n",
        "\n",
        "print(f\"Values of real-valued features in the first {num_rows_to_inspect} rows:\")\n",
        "\n",
        "# Iterate through the selected rows\n",
        "for i in range(sample_rows.shape[0]):\n",
        "    print(f\"\\nRow {i+1}:\")\n",
        "    # Iterate through the real-valued feature indices\n",
        "    for feature_index in sorted(list(real_valued_feature_indices)): # Sorting for consistent output\n",
        "        # Check if the feature exists in the current row (i.e., it's non-zero)\n",
        "\n",
        "        if feature_index in sample_rows[i].indices:\n",
        "            # Get the index within the non-zero elements\n",
        "            data_index = np.where(sample_rows[i].indices == feature_index)[0][0]\n",
        "            # Get the value of the feature\n",
        "            feature_value = sample_rows[i].data[data_index]\n",
        "            print(f\"  Feature {feature_index}: {feature_value}\")\n",
        "        # If the feature index is not in sample_rows[i].indices, its value is 0 in the sparse matrix,\n",
        "        # so we don't need to explicitly print 0 unless we want to see all real-valued features\n",
        "        # even if their value is 0 for that sample. Let's only print non-zero real-valued features."
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Values of real-valued features in the first 5 rows:\n",
            "\n",
            "Row 1:\n",
            "  Feature 4: 0.124138\n",
            "  Feature 5: 0.117647\n",
            "  Feature 16: 0.749633\n",
            "  Feature 17: 0.843029\n",
            "  Feature 18: 0.197344\n",
            "  Feature 21: 0.142857\n",
            "  Feature 22: 0.142857\n",
            "  Feature 55: 1.0\n",
            "  Feature 63: 1.0\n",
            "  Feature 69: 1.0\n",
            "  Feature 71: 1.0\n",
            "  Feature 73: 1.0\n",
            "  Feature 75: 1.0\n",
            "  Feature 81: 1.0\n",
            "  Feature 83: 1.0\n",
            "  Feature 85: 1.0\n",
            "  Feature 87: 1.0\n",
            "  Feature 89: 1.0\n",
            "  Feature 91: 1.0\n",
            "  Feature 93: 1.0\n",
            "  Feature 95: 1.0\n",
            "  Feature 101: 1.0\n",
            "  Feature 103: 1.0\n",
            "  Feature 105: 1.0\n",
            "  Feature 107: 1.0\n",
            "  Feature 109: 1.0\n",
            "  Feature 111: 1.0\n",
            "\n",
            "Row 2:\n",
            "  Feature 4: 0.103448\n",
            "  Feature 5: 0.176471\n",
            "  Feature 16: 0.72266\n",
            "  Feature 17: 0.836498\n",
            "  Feature 18: 0.6189\n",
            "  Feature 21: 0.0119048\n",
            "  Feature 23: 1.0\n",
            "  Feature 55: 1.0\n",
            "  Feature 61: 1.0\n",
            "  Feature 63: 1.0\n",
            "  Feature 65: 1.0\n",
            "  Feature 67: 1.0\n",
            "  Feature 69: 1.0\n",
            "  Feature 71: 1.0\n",
            "  Feature 73: 1.0\n",
            "  Feature 75: 1.0\n",
            "  Feature 81: 1.0\n",
            "  Feature 83: 1.0\n",
            "  Feature 85: 1.0\n",
            "  Feature 87: 1.0\n",
            "  Feature 89: 1.0\n",
            "  Feature 91: 1.0\n",
            "  Feature 101: 1.0\n",
            "  Feature 103: 1.0\n",
            "  Feature 105: 1.0\n",
            "  Feature 107: 1.0\n",
            "  Feature 109: 1.0\n",
            "  Feature 111: 1.0\n",
            "  Feature 126: 1.0\n",
            "  Feature 150: 1.0\n",
            "\n",
            "Row 3:\n",
            "  Feature 4: 0.144828\n",
            "  Feature 5: 0.117647\n",
            "  Feature 16: 0.760482\n",
            "  Feature 17: 0.820882\n",
            "  Feature 18: 0.150678\n",
            "  Feature 21: 0.142857\n",
            "  Feature 23: 1.0\n",
            "  Feature 43: 1.0\n",
            "  Feature 55: 1.0\n",
            "  Feature 61: 1.0\n",
            "  Feature 63: 1.0\n",
            "  Feature 65: 1.0\n",
            "  Feature 67: 1.0\n",
            "  Feature 69: 1.0\n",
            "  Feature 71: 1.0\n",
            "  Feature 73: 1.0\n",
            "  Feature 75: 1.0\n",
            "  Feature 81: 1.0\n",
            "  Feature 83: 1.0\n",
            "  Feature 85: 1.0\n",
            "  Feature 87: 1.0\n",
            "  Feature 89: 1.0\n",
            "  Feature 91: 1.0\n",
            "  Feature 93: 1.0\n",
            "  Feature 95: 1.0\n",
            "  Feature 101: 1.0\n",
            "  Feature 103: 1.0\n",
            "  Feature 105: 1.0\n",
            "  Feature 107: 1.0\n",
            "  Feature 109: 1.0\n",
            "  Feature 111: 1.0\n",
            "\n",
            "Row 4:\n",
            "  Feature 4: 0.0896552\n",
            "  Feature 5: 0.176471\n",
            "  Feature 21: 0.0238095\n",
            "  Feature 23: 1.0\n",
            "  Feature 35: 1.0\n",
            "  Feature 43: 1.0\n",
            "  Feature 55: 1.0\n",
            "  Feature 61: 1.0\n",
            "  Feature 63: 1.0\n",
            "  Feature 65: 1.0\n",
            "  Feature 67: 1.0\n",
            "  Feature 69: 1.0\n",
            "  Feature 71: 1.0\n",
            "  Feature 73: 1.0\n",
            "  Feature 75: 1.0\n",
            "  Feature 81: 1.0\n",
            "  Feature 83: 1.0\n",
            "  Feature 85: 1.0\n",
            "  Feature 87: 1.0\n",
            "  Feature 89: 1.0\n",
            "  Feature 91: 1.0\n",
            "  Feature 93: 1.0\n",
            "  Feature 95: 1.0\n",
            "  Feature 101: 1.0\n",
            "  Feature 103: 1.0\n",
            "  Feature 105: 1.0\n",
            "  Feature 107: 1.0\n",
            "  Feature 109: 1.0\n",
            "  Feature 111: 1.0\n",
            "  Feature 150: 1.0\n",
            "\n",
            "Row 5:\n",
            "  Feature 4: 0.131034\n",
            "  Feature 5: 0.117647\n",
            "  Feature 16: 0.830283\n",
            "  Feature 17: 0.83965\n",
            "  Feature 18: 0.583194\n",
            "  Feature 19: 1.0\n",
            "  Feature 21: 0.00595238\n",
            "  Feature 22: 0.00595238\n",
            "  Feature 35: 1.0\n",
            "  Feature 43: 1.0\n",
            "  Feature 55: 1.0\n",
            "  Feature 61: 1.0\n",
            "  Feature 63: 1.0\n",
            "  Feature 65: 1.0\n",
            "  Feature 67: 1.0\n",
            "  Feature 69: 1.0\n",
            "  Feature 71: 1.0\n",
            "  Feature 73: 1.0\n",
            "  Feature 75: 1.0\n",
            "  Feature 81: 1.0\n",
            "  Feature 83: 1.0\n",
            "  Feature 85: 1.0\n",
            "  Feature 87: 1.0\n",
            "  Feature 89: 1.0\n",
            "  Feature 91: 1.0\n",
            "  Feature 93: 1.0\n",
            "  Feature 95: 1.0\n",
            "  Feature 101: 1.0\n",
            "  Feature 103: 1.0\n",
            "  Feature 105: 1.0\n",
            "  Feature 107: 1.0\n",
            "  Feature 109: 1.0\n",
            "  Feature 111: 1.0\n",
            "  Feature 132: 1.0\n",
            "  Feature 138: 1.0\n",
            "  Feature 140: 1.0\n",
            "  Feature 142: 1.0\n",
            "  Feature 144: 1.0\n",
            "  Feature 146: 1.0\n",
            "  Feature 148: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eb21fdfb"
      },
      "source": [
        "# Task\n",
        "Balance the classes in the `combined_y` variable of the dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e12ae549"
      },
      "source": [
        "## Check class distribution\n",
        "\n",
        "### Subtask:\n",
        "Analyze the current distribution of the target variable (`combined_y`) to see how imbalanced the classes are.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aed82c52"
      },
      "source": [
        "**Reasoning**:\n",
        "Calculate and print the counts of each unique class in the `combined_y` array to understand the class distribution.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5bb1f3a4",
        "outputId": "de365a22-a815-4660-f386-1d7985ae2c44"
      },
      "source": [
        "unique_classes, class_counts = np.unique(combined_y, return_counts=True)\n",
        "\n",
        "print(\"Class distribution in combined_y:\")\n",
        "for class_val, count in zip(unique_classes, class_counts):\n",
        "    print(f\"Class {int(class_val)}: {count}\")"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class distribution in combined_y:\n",
            "Class -1: 1603985\n",
            "Class 1: 792145\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "50ad112b"
      },
      "source": [
        "## Choose a balancing technique\n",
        "\n",
        "### Subtask:\n",
        "Decide on an appropriate method for balancing the classes, such as oversampling (e.g., SMOTE), undersampling, or a combination.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "562c76ef"
      },
      "source": [
        "## Apply the balancing technique\n",
        "\n",
        "### Subtask:\n",
        "Apply the balancing technique\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dadc4dfd"
      },
      "source": [
        "**Reasoning**:\n",
        "To balance the dataset, I will first import the necessary libraries, `SMOTE` and `RandomUnderSampler`. Then, I will apply SMOTE to oversample the minority class, followed by RandomUnderSampler to undersample the majority class, as per the instructions. This will create a more balanced dataset for model training.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "id": "24022c39",
        "outputId": "d484e6dd-4d48-4dc8-9e12-177891865462"
      },
      "source": [
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "\n",
        "# 1. Instantiate SMOTE\n",
        "smote = SMOTE(sampling_strategy=0.5, random_state=42)\n",
        "\n",
        "# 2. Apply SMOTE to the data\n",
        "X_smote, y_smote = smote.fit_resample(combined_X, combined_y)\n",
        "\n",
        "# 3. Instantiate RandomUnderSampler\n",
        "rus = RandomUnderSampler(sampling_strategy=0.8, random_state=42)\n",
        "\n",
        "# 4. Apply RandomUnderSampler to the SMOTE-resampled data\n",
        "X_resampled, y_resampled = rus.fit_resample(X_smote, y_smote)\n",
        "\n",
        "# Print the shapes of the resampled data to verify\n",
        "print(f\"Shape of X after SMOTE and RandomUnderSampler: {X_resampled.shape}\")\n",
        "print(f\"Shape of y after SMOTE and RandomUnderSampler: {y_resampled.shape}\")\n",
        "\n",
        "# Verify the new class distribution\n",
        "unique_classes_resampled, class_counts_resampled = np.unique(y_resampled, return_counts=True)\n",
        "print(\"\\nClass distribution after resampling:\")\n",
        "for class_val, count in zip(unique_classes_resampled, class_counts_resampled):\n",
        "    print(f\"Class {int(class_val)}: {count}\")"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'combined_X' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1-3737122931.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# 2. Apply SMOTE to the data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mX_smote\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_smote\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmote\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_resample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcombined_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcombined_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# 3. Instantiate RandomUnderSampler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'combined_X' is not defined"
          ]
        }
      ]
    }
  ]
}